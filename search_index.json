[
["index.html", "Quantitative Political Methods Coursebook 1 Class Overview 1.1 Learning objective #1: What is the point of all of this group work? 1.2 Learning object #2: What is the point of all of these online activites? 1.3 Learning objective #3: How is this going to work?", " Quantitative Political Methods Coursebook Jacob Montgomery 1 Class Overview 1.1 Learning objective #1: What is the point of all of this group work? This year, this course has been revised to incorporate some elements of Team Based Learning. This is an approach to creating permanent groups to work on problems in class, in lab, and out of class throughout the semester. We are not going to follow the TBL approach strictly. But we are going to have fewer lectures and more structured group activities. Learn more about Team Based Learning by watching this video. 1.2 Learning object #2: What is the point of all of these online activites? A better question would be what is the point of sitting in a room and watching a professor talk? It’s 2012! Moving an increasing amount of teaching resources online is a big trend in education at all levels. But this isn’t just trendy, it appears to be very effective. Preliminary research shows that online learning paired with in-class interactions with faculty is a much more effective than the traditional lecture format. Watch this short TED talk by Peter Norvig to learn more: 1.3 Learning objective #3: How is this going to work? The approach I am trying to adopt for this class is to help you learn in four basic steps. Initial exposure to materials through self-study. Repeated exposure to materials in short lectures. Use your new knowledge in a collaborative environment where assistance is readily available. Use your new knowledge on your own. The goal is for you to see and use information in multiple ways to improve learning outcomes. Experience (and research) shows that this approach is far superior to simple lectures in helping you learn more and retain it longer. To help you along this process, each learning component will be broken down as follows. You will read your book and review online materials. Regular online quizzes will help motivate you to stay current. I will begin most classes with a short lecture where I will cover the materials again and answer questions. You will apply your knowledge during in-class team assignments. If you have any questions or run into problems, I will be right there to give you help. To keep the team motivated, these assignments will be graded. You will apply your knowledge on your own (or with friends) in your problem sets. "],
["measurement-error.html", "2 Measurement Error 2.1 Learning Objectives 2.2 What’s the takeaway?", " 2 Measurement Error 2.1 Learning Objectives Learn to define the following types of errors: sampling bias, response bias, non-response bias ##Learning Objective 1: Types of Biases If the goal of a survey is to be representative, then anything that causes a survey to deviate from the population is a major problem. Something that causes a survey result to deviate from the population is referred to as bias. Bias can be introduced in a number of ways. We will divide biases into three categories: sampling bias, response bias, and non-response bias. 2.1.1 Sampling bias Sampling bias occurs when there is a problem in the sampling process. Sometimes, the sample is poorly conducted. For example, in a survey of WashU undergraduates, if we put together a stratified sample, based on residential area, and then selected 50% of our subjects from Shepley Hall, then our survey would not be reflective of the general WashU undergraduate population, because 50% of WashU students do not live in Shepley hall. This is a sampling error. To give a political example, in telephone surveys, sometimes people do not pick up their phone. In this situation, most polling firms will try calling back later. If a firm does not try to call back when people are out, then they run the risk of introducing sampling error, because younger people are more likely to be out than older people, and younger people are more likely to be liberal. Other times, a sample simply isn’t random. Certain groups are systematically left out of a sample. This is called a selection bias, or selection effect. In this case, the sample will fail to reflect the population as a whole. Although this may sound like an obvious problem, this happens all the time. For example, hosts at various cable news shows sometimes ask viewers to text in what they think about a certain policy, and then display the results as if they are reflective of the general population. But just because 93% of people who texted in think that the President should be re-elected does not mean that the population as a whole agrees. Since it is not a randome sample, it doesn’t even mean that 93% of viewers agree! A common situation where sampling error occurs involves phone lines. People without phone lines are excluded from most surveys. That may not be too big of an issue in a country like the United States, where there may not be a substantial difference between people with phone lines, and people without them. But in a country like Afghanistan relying on phone calls to conduct a survey would be a very big problem since phones are only owned by (relatively) wealthy individuals and some areas have no service at all! Another example is Geico commercials, where they say that people who switch to Geico on average save money. This is a selection bias, because only people who are offered a cheaper rate will switch. If people were randomly selected and forced to switch to Geico, then they would not be saving 30% or more. 2.1.2 Response bias In some surveys, the questions may be poorly worded. The language may be ambiguous, or may be open to different interpretations. So the process of answering the questions may lead to some bias in how the questions are answered. In some surveys, something about the survey itself causes subjects to respond differently than they would in real life. For example, if someone is being surveyed on how often they volunteer, and (s)he rarely volunteers, (s)he may feel embarrassed or guilty about this fact, and tell the interviewer that (s)he volunteers frequently. This is called the social desirability effect. For example, if a survey asks someone if they voted in the last election, and they had not, that person may be more inclined to answer untruthfully, because failing to vote is frowned upon. Other times, the ordering of questions might affect responses. For example, if a survey begins by asking subjects numerous questions about terrorism, and then asks them to list the issues most important to them, more subjects might list terrorism than would be the case if the survey had not first asked numerous questions about it. This is referred to as order effect. An example of order effect occurred in a survey in 1948 that asked, Q1: Do you think the US should let Communist newspaper reporters from other countries come in here and send back to their papers the news as they see it? But half of the people were first asked, Q2: Do you think a communist country like Russia should let American newspaper reporters come in and send back to America the news as they see it? Among people who were first asked the question about American reporters (Q2), 73.1% said yes to Q1. But, among people who did not first answer Q2, only 36.5% said yes to Q1. 2.1.3 Non-response bias People don’t always respond to surveys. When people who do not respond to surveys differ in some systematic way from people who do respond, then the sample fails to be representative of the general population. For example, students who submit course evaluations tend to do so because they either disliked their professor, or loved their professor. In a class, students who do not have strong feelings one way or another are less likely to submit an evaluation, so the evaluation results are likely to indicate that the class is more polarized than it actually is. Note: Non-response bias is not the same as selection bias. Selection bias has to do with how the sample was constructed by the researcher. Non-response bias has to do with how individuals chosen to be in the sample behave. 2.2 What’s the takeaway? There are several different ways that a survey can fail to reflect the population. The sample itself could not be reflective of the population, which would be a sampling bias. Sometimes, something about the survey could prompt people to give answers that don’t reflect what they actually believe or do, which is a response bias. Other times, the people who are selected, but don’t respond differ in an important way from people who do respond, causing the results of the survey to be skewed. This is called a non-response bias. "],
["scales-of-measurement.html", "3 Scales of Measurement 3.1 Learning Objectives: 3.2 Learning Objective 1: Scales 3.3 Summary: 3.4 Learning Objective 2: Granularity 3.5 What is granularity? 3.6 What are the takeaways?", " 3 Scales of Measurement 3.1 Learning Objectives: Learn what ordinal, nominal, and interval scales are. Be able to distinguish between them. Learn what “granularity” means, learn what the two types of granularity, continuous and discrete, mean, and learn how to distinguish them 3.2 Learning Objective 1: Scales 3.2.1 First of all, what do we mean by scales? A “scale” is just the way we measure or quantify a variable. There are three different scales with which data can be measured: nominal, ordinal, or interval. 3.2.2 Nominal Scale Variables measured on a nominal scale can be separated into different categories, but they have no natural ordering. We can’t say that one data point is “more something” than another point. One example is conenent. It is possible to put countries into different categories based on the continent they’re in. The reason that continent is a nominal variable is that, while we can put countries into different categories, there is no universal, objective way of putting them along a dimension. Note: Sometimes variables measured on a nominal scale are called “categorical” data. 3.2.3 Ordinal Scale: Variables measured on an ordinal scale have a natural ordering, but no natural distances between them. We can put the data points in a conceptual line, but there isn’t a precise (or meaningful) distance between observed values. Many survey questions measure opinion on an ordinal scale. For example, in 2012 The Pew research Center asked: Is your overall opinion of Barack Obama very favorable, mostly favorable, mostly unfavorable, or very unfavorable? We can be pretty sure that people who respondend “very unfavorable” approved less of President Obama than people who responded “mostly unfavorable.” But how much less? We don’t know exactly, which is why this variable is measured on an ordinal scale. (You can see many more examples of question wordings to measure approval Preisdent Obama here.) 3.2.4 Interval Scale: Variables measured on an interval scale have both a clear ordering and the difference between numbers has a clear meaning. We can not only put data points in a line, we can also position them within precise distances from each other. An easy example is height. We can order everyone in QPM in a line, from tallest to shortest, and we can say precisely how much taller (or shorter) one person is than the others. 3.2.5 Ratio scale: Ratio scale is a subset of Interval data. In a ratio scale, the zero value signifies that there is none of that variable. Most scales in social science are ratio scales. For example, if we were to measure the proportion of the time a legislator votes with the leadership of a certain party, then a score of 0% means that that legislator cast no votes with the party leadership. An example of a scale that is not measured on a ratio scale is temperature measured in Fahrenheit or Celsius. If a room is 0 degrees Fahrenheit or Celsius, that does not mean that there is no warmth. 3.2.6 Focus on the measurement and not the concept Certain variables, when measured in different ways, can be measured with different scales. For example, think about political ideology. One way to measure the ideology of a member of Congress would be to ask them if they are “Very liberal, somewhat liberal, somewhat conservative, or very conservative.” This would have an ordinal scale. However, there are other ways to measure ideology. For example, we could measure the proportion of votes each member cast that were consistent with the Republican leadership. This would allow us to make a scale of ideology that is interval. (We might find that Nancy Pelosi’s voting record is 70.2% more liberal than John Boehner’s.) Below is the link to Congressional rankings released by Americans for Democratic Action, a left-wing political group, and rankings released by the Club for Growth, and right-wing political group. http://www.adaction.org/media/votingrecords/2010.pdf http://www.clubforgrowth.org/projects/ 3.3 Summary: Watch this video for a very nice summary of these concepts and some thoughts on how best to plot them. 3.4 Learning Objective 2: Granularity 3.5 What is granularity? Granularity refers to how answers fit on a scale. If a variable can take on any value along a scale, it is continuous. An example of a continuous variable is the proportion of the vote a candidate receives in an election. A candidate can receive 0%, 100%, or any value in between. If a variable can only take on certain (countable) values, it is discrete. All nominal data are discrete. Consider the following survey question, asked in a poll by Rusmussen Reports (full survey here): A proposal has been made to repeal the health care law and stop it from going into effect. Do you strongly favor, somewhat favor, somewhat oppose or strongly oppose a proposal to repeal the health care law? In the above question, the results would be discrete, because there are only 5 possible answers. Respondents can strongly favor, somewhat favor, some oppose, or strongly oppose the proposal. There is no possible answer between strongly favor or somewhat favor, for example. Here is a video that can help you tell the difference between discrete and continuous data. 3.6 What are the takeaways? Based on whether data have a natural order or natural distance, they can be measured on a nominal, ordinal, or interval scale. A specific type of interval scale is a ratio scale. Some variables, measured in different ways, can potentially be measured on several types of scales. Based on the space between answers, or granularity, scales can either be continuous, if variables can take on any value on the scale, or discrete, if they can only take on certain values. This video will help you keep this all straight. "],
["random-samples.html", "4 Random Samples 4.1 Learning Objectives 4.2 Learning Objective 1: Random Samples 4.3 Learning Objective 2: Types of Random Samples 4.4 What are the takeaways?", " 4 Random Samples 4.1 Learning Objectives 1)Understand what a random sample is, and why random samples are important. 2)Identify simple random samples, systematic random samples, stratified random samples, cluster samples, and multi-stage sampling, and understand why scientists use them. 4.2 Learning Objective 1: Random Samples 4.2.1 What is a random sample? A random sample is a sample where every subject in the population in question has an equal chance of being selected. If there are 6,000 undergrads at WashU, and I want to select 600 to be in my sample, then if my method of selecting subject is random, each undergrad has the same chance of being selected, 10%. 4.2.2 Why do we care if a sample is random? We want our samples to be representative. In other words, we want our sample to look like the population we’re drawing from, just smaller. If we are conducting a study on how happy they are with their dorm, and 10% of WashU undergrads live in a “traditional” dorm on the South 40, then we want 10% of our sample to be comprised of students living in a traditional dorm on the South 40. More technically, random samples are representative in expectation. That means that if we took a lot of different random samples, on average they would be representative. So while any one random sample might be off just a bit, random samples in general will be representative. 4.2.3 Real-life application: Don’t look ignorant in your newspaper column Below is the link to an article by Rodger Simon, Politico’s chief political columnist. In this article, he dismisses polls because they ask so few people what they think. This is a perfectly reasonable argument to make if one has had no background in statistics. What Roger Simon failed to grasp, and the reason why polls work, even though they ask such few people, is that, when they are done well, they are representative of the population as a whole. http://www.politico.com/news/stories/1211/70717.html 4.3 Learning Objective 2: Types of Random Samples There are several different types of random samples. In each of these samples, every subject in the population has the same probability of being selected, but the subjects are selected in different ways. 4.3.1 Simple Random Sample: This is the most basic method of sampling. A certain number of subjects are chosen randomly out of a population. An example of this is lotteries. In a lottery, each number has the same chance of being selected. Here’s a helpful video that explains simple random samples: If simple random samples are representative, why not just use them for all samples? The problem is that it is extremely difficult, if not impossible, to put together a simple random sample for large populations. One challenge is that certain groups have different response rates than other groups. In this case, if we just do a simple random sample, then groups that have a higher response rate will be over-represented. Another challenge is that there is no directory with the names and contact information of every single person, so it can be difficult to put together a sample in the first place. Scientists employ more complex sampling methods to remedy these problems. 4.3.2 Systematic Random Sample: In a systematic sample, subjects are selected through some sort of system. For example, I might collect the student id numbers of all WashU undergrads, order them from least to greatest, and then start with the 8th number, and sample every 10th person. This is still a random sample, because each person has the same chance of being selected. It’s not, however, a simple random sample, because instead of being selected entirely randomly in no order, there was a system to choosing the people. In a simple random sample, every set of people has the same chance of being in the sample. In a systematic random sample, that’s not true. For example, if my student ID number is 1 higher than my roommate’s, then the in a simple random sample it’s possible that both my roommate and I will be in the sample. In the systematic random sample I just described, however, it is impossible that my roommate and I will be in the same sample. Everyone in the sample has a student id number that’s 10 apart. Here’s a helpful video that explains systematic random samples: 4.3.3 Stratified Random Sample: In a Stratified Random Sample, we first split the population into different groups, or strata. We then select a set number of subjects from each group. The goal is to set up the samples such that the proportion of subjects in the sample from each strata match the proportion of subjects from each strata in the population. To return to the example of sampling the undergraduate population of WashU, we might divide undergrads into different strata based on their residential area. If 5% of all WashU undergrads live in Mudd Hall, and we want our sample to be 600 people, then 5% of those 600 people, or 30 people, will be from Mudd Hall. To achieve this, we would do a simple random sample of Mudd residents, choosing 30 subjects. And we would do this for each residential area. Here’s a helpful video that explains stratified random sampling: What are the advantages of a stratified random sample? Stratified Random Samples can be used to remedy problems of over-sampling, or under-sampling. If different groups have different rates of response, then we can select more subjects from those groups, so the set of people who respond are representative. For example, if freshmen respond much less to surveys than upper-classmen, then we might use a stratified random sample, and over-sample freshmen, to correct for this. 4.3.4 Cluster Samples: In a cluster sample, the population is divided into different clusters. These are organic groupings of people. A simple random sample is run to select several clusters, and then everyone in each selected cluster is surveyed. For example, in the WashU undergrad survey, we might “cluster” students by floor. Wheeler 1 would be a cluster, Lopata 3 another, and so on. We would randomly select a few floors, and interview everyone on each floor. Here’s a helpful video that explains cluster sampling: What are the advantages of a cluster sample? In some ways, cluster samples are easier to execute. If many of the people we are interviewing live close together, it might be easier to reach them. Of course, cluster sampling is not without its drawbacks. Because subjects in the same cluster tend to be similar in certain systematic ways, the cluster sampling can lead to over-sampling of certain types of people. 4.3.5 Multi-Stage Sampling: A multi-stage sample employs several types of random samples. For example, for the undergrad survey, we might first stratify WashU undergrad by whether they live on the South 40, in the Village, or off campus. We might then do a cluster sample within each strata. We could choose 4 floors from each strata, and interview each person on that floor. 4.4 What are the takeaways? Random sampling is the way scientists develop representative samples. There are several different ways of putting together random samples. These include cluster samples, stratified random samples, systematic samples, and multi-stage samples, which combine aspects of different sampling methods. "],
["installing-r-and-r-studio.html", "5 Installing R and R Studio 5.1 Installing R and R Studio on a Mac (would also work for other platforms) 5.2 Installing R and R Studio on Windows", " 5 Installing R and R Studio 5.1 Installing R and R Studio on a Mac (would also work for other platforms) 5.2 Installing R and R Studio on Windows "],
["a-brief-introduction-to-r.html", "6 A Brief Introduction to R", " 6 A Brief Introduction to R Learning objectives: 1)Learn how to assign a value to an object in R. 2)Learn your way around R (using R Studio). 3)Learn how to do simple arithmetic in R. "],
["measures-of-position.html", "7 Measures of Position 7.1 Percentile 7.2 Interquartile Range 7.3 Outliers 7.4 Skew", " 7 Measures of Position Learning Objectives: Understand and the terms percentale, interquartile range, outliers and skew. 7.1 Percentile The nth percentile means that n% of observations fall below (or are equal to) n, and (100-n)% fall above n. For example, if you score in the 90th percentile on a test, that means that 90% of students who took the test earned the same or a lower score than you, and 10% scored higher than you. The percentile tells us a lot about a specific observation within a data set because it gives us its relative position. The median is the 50th percentile because exactly 50% of observations fall above the median, and 50% of observations fall below the median. 7.2 Interquartile Range This way of looking at data splits observations into four parts (four quarters). The Interquartile Range (IQR) determines the middle 50% of the data (the middle two quarters). IQR= (75th percentile-25th percentile) The 25th percentile is the lower quartile (25% of data falls below the 25th percentile) and the 75th percentile is the upper quartile (25% of data falls above the 75th percentile). A good way of finding these observations in an ordered set of data is to determine the median, then find the median from the highest data point to the median, and the lowest data point to the median. For example, if you had the data 1 2 3 4 5, you could determine by counting in 3 numbers from each side that the median is 3. You could then find the median of 1 2 3 and of 3 4 5. That will tell you that 2 is the value of the lower quartile, and 4 is the value of the upper quartile. 4-2= 2, which means that the IQR= 2. 7.3 Outliers An outlier is an observation whose value is so much greater or lesser than the other observations that it can be considered an extreme. For example, if you had the data 1 2 3 4 8000, you can easily tell that 8000 is an outlier because it is so much larger than the other values. Although there are many definitions, one common definition of an outlier is any observation that falls 1.5(IQR) above the upper quartile, or 1.5(IQR) below the upper quartile. In our above data set, we can determine that the IQR=2. 1.5(2)=3. This means that any value less than -1 or greater than 7 is an outlier, so 8000 is clearly an outlier. 7.4 Skew The skew tells us the tendency of the data. To determine skew, you can look at a histogram of the data, or compare the median and the mean of a set of data. If the longer tail of the data is to the left of the mode, the data is skewed to the left. We call this a negative skew. If that longer tail of the data is to the right, the data is skewed to the right; a positive skew. In comparing the median and the mean, if the mean is greater than the median, the data is positively skewed, and if the mean is smaller than the median, the data is negatively skewed. We can compare these two measures of center because the mean is more sensitive to outliers than the median. For more information about skew, check out this video: //Theres supposed to be a video embedded here but I cannot find it; will fix later "],
["measures-of-dispersion.html", "8 Measures of Dispersion 8.1 Learning objective 1: Range 8.2 Learning objective 2: Deviations 8.3 Learning objective 3: Variance 8.4 Learning objective 4: Standard deviation 8.5 Learning objective 5: The Empirical Rule", " 8 Measures of Dispersion Learning Objectives: Understand how to calculate the range of a variable and how to calculate it. Understand and be able to calculate the deviation of specific data point. Understand how to calculate the variance of variable. Understand how to calculate the standard deviation of a variable, and its relationship to the variance. Understand the “Empirical Rule” and how it is related to the standard deviation. 8.1 Learning objective 1: Range The range is a simple measure of variability that shows how spread out the observations in a data set are. Range= (largest observation - smallest observation). Because the range is a measure of distance, the value of the range is always positive. The range really doesn’t tell us anything much that is useful. 8.2 Learning objective 2: Deviations A deviation is a measure of distance from a measure of center, \\[|x_1 - \\bar{x}|\\]. The observation may be greater or lesser than the value of the center, but because deviation is a measure of distance, its value is always positive. 8.3 Learning objective 3: Variance Variance is the average of squared deviations. The value of the variance is a squared value (for instance, inches squared, percentage squared, children squared) which does not necessarily tell us very clearly about how spread out the data is from the mean. Variance is calculated by dividing the sum of squared deviations from the value of the population by \\(n-1\\). This is the equation: \\[s^2=\\frac{\\sum_i^n (x_i - \\bar{x})^2}{n-1} \\] When variance increases it means the observations are more spread out from the mean. 8.4 Learning objective 4: Standard deviation The standard deviation is the square root of the variance. \\[s=\\sqrt{s^2}=\\sqrt{\\frac{\\sum_i^n (x_i -\\bar{x})^2}{n-1}}\\] Like the variance, the standard deviation is always a positive number because it represents a distance from the mean. When x is a constant (for example, you have the observations 3 3 3 3 3), the standard deviation is 0 because there is no deviation from the mean. The standard deviation increases as variability increases around the mean. NOTE: The standard deviation is greatly affected by outliers. 8.5 Learning objective 5: The Empirical Rule The Empirical Rule is the distribution of observations is the histogram of the data is approximately bell shaped; if it is a normal, symmetrical distribution. The Rule says that about 68% of observations ball between \\(\\bar{x}-s\\) and \\(\\bar{x}+s\\); that about 95% of observations fall between \\(\\bar{x}-2s\\) and \\(\\bar{x}+2s\\); and that nearly all the data in a normal distribution will fall between \\(\\bar{x}-3s\\) and \\(\\bar{x}+3s\\). The Empirical Rule only works if the data is approximately bell shaped. It does not apply to data that is skewed. For more information about measures of dispersion, check out this video: //Should have a video? "],
["measures-of-central-tendency.html", "9 Measures of Central Tendency 9.1 Learning objective 1: Mean 9.2 Learning objective 2: Median 9.3 Learning objective 3: Mode", " 9 Measures of Central Tendency Learning Objectives: Learn what a mean is and how to calculate one. Learn what a median is and how to calculate one. Learn what a mode is and how to calculate one. Understand how all of alternative measures differ and why. 9.1 Learning objective 1: Mean $x_1 $ indicates a single observation, or data point. \\(\\bar{x}\\) represents the mean of all the observations, which is taken by adding up all the observations and dividing them by the number of observations. We call this the arithmetic mean, or average. More formally, the equation for the mean is: \\[\\bar{x} = \\frac{\\sum_i^n x_i }{n} \\] The mean is used only for quantitative data and interval data. (For example, if you were to ask respondents on a survey to circle their preferred baseball team, the Cardinals, Cubs, Reds, or Pirates, you couldn’t then add up the teams and divide by four.) The mean is highly influenced by outliers, which means that for skewed distributions of data, the mean lies in the direction of the skew. The trimmed mean is a way of correcting for the effect of outliers on the mean. It involves cutting out a certain percentage of the data from both extremes. For example, a 10% trimmed mean involves throwing out the top 10% and bottom 10% of findings, then calculating the mean from the remaining middle 80% of the data. 9.2 Learning objective 2: Median The median is a measure of central tendency found by determining the middle value of a set of data. The value of the median is the 50th percentile because 50% of observations fall above the value of the median, and 50% of observations fall below the median. The median it is the middle of an ordered sequence. For example, if we look at the numbers 1 2 3 4 5, the middle of that ordered sequence is 3. If we have an even number of observations, say 1 2 3 4 5 6, we take the mean of the two middle numbers. So the median here is equal to 3.5. The median is used for quantitative, ordinal or interval data. For symmetric distributions (e.g., a normal bell curve), the mean and the median are identical. However, the median is not affected by outliers like the mean is, so if a distribution of data is highly skewed, the median tends to be the preferred measure of center over the mean. This image shows three graphs two skewed and one symmetrical bell curve. Note the way the mean is easily influenced by the skew of the graph, how it is pulled towards the longer tail of a distribution. Comparing the mean and the median is a good way to determine the skew of data without actually making a histogram. Source: http://experimentaltheology.blogspot.com/2012/03/central-tendency-in-skewed.html 9.3 Learning objective 3: Mode The mode is a measure of central tendency that is the value that occurs most frequently. The mode is the most common data point. For example, if you collected the data 1 2 2 3 4, the mode is the number 2. Imagine a bar graph of the data. The mode is whichever bar is the tallest. The mode is used for all types of data. When data is bimodal, which means that there are two distinct values that tend to occur more frequently (in a histogram, this would mean there are two lumps in the graph), it can indicate polarization around two extremes. For symmetric distributions, the mean, median, and mode are all the same value. This histogram comes from author Keith T. Poole using the W-NOMINATE dataset. Poole and his fellow researchers gave legislators “ideological points” based on past votes to determine where that legislator falls on the political spectrum. The above graph shows one mode for the Democrats, which falls between liberal and moderate, and another mode for the Republicans between moderate and conservative. This should make sense given what we know about modern American politics. The polarization of Congress- that is to say, the Democrats are leaning more left of center than in previous years and the Republicans trending more right of center, is illustrated by this histogram. Source: http://voteview.com/nominate/nominate.htm Another look: For more information about measures of center, check out this video: "],
["visualizing-data-in-r.html", "10 Visualizing Data in R", " 10 Visualizing Data in R Learning Objectives: Learn how to create simple plots in R. Learn how to interpret the results. "],
["importing-data-into-r.html", "11 Importing Data into R", " 11 Importing Data into R Learning Objectives: Learn how to import data into R. Learn how to look at the data. "],
["frequency-and-probability-distributions.html", "12 Frequency and Probability Distributions 12.1 What is a probability? 12.2 Frequency distribution", " 12 Frequency and Probability Distributions A General Introduction Learning Objectives: 1)Define probability. 2)Understand the basic properties of a probability. 3)Understand the meaning of “frequency distribution.” 12.1 What is a probability? There are many definitions of a probability, but here is one that is easy to understand and that is pretty useful for this class is the frequency interpretation of probability. In essence, we imagine doing something (flipping a coin, rolling a dice, taking a random sample) Probability (frequency interpretation) The relative frequency of occurrence for some particular outcome if a process is repeated a large number of times under similar conditions. Probabilities help us answer questions like: If I flip a coin three times, what is the probability that I will get exactly two heads? If I roll two dice, what is the probability of getting a two? If I take a random sample of 100 Wash U students, what is the probability that less than 40% of the sample will be male? 12.2 Frequency distribution A probability distribution of a discrete variable, Y, assigns a probability to each possible outcome. As an example, we can write out the frequency distribution for all possible outcomes of rolling two fair dice. In the left column, is the outcome. In the right column is the probability of observing that outcome. \\(S= {y_1+ y_2 +\\ldots+y_k}\\), which means that S represents the set of all possible outcomes in a set of data, Y is the actual realization of the variable. Example: if we flipping a coin three times, S is the set of possible outcomes (0 heads, 1 head, 2 heads, 3 heads). The variable (the number of heads we observe from flipping a coin three times) is represented by the variable Y. \\(p(y_k)= Pr(Y=y_k)\\) where \\(0 \\le p(y_k) \\le 1 \\forall k\\) This looks complicated, but essentially what it means is that the probability of an event occurring equals the probability of a specific event out of all of the other observations. This value falls between 0 and 1 no matter what. An occurrence cannot be more than 100% likely to occur or less than 0% likely not to occur, so the proportion will always be between 0 and 1. The closer it is to 1, the more likely it is that the predicted event will occur. $ _{k=1}^K p(y_k )=1$ This means that the sum of all the probabilities that an event will occur equals 1. Here are some videos to help: This last video walks you through the construction of a probability density function. "],
["some-basic-plotting-in-r.html", "13 Some basic plotting in R", " 13 Some basic plotting in R We have made some videos to help you do some basic plots. These were made especially to help you with the problem set. How to plot a normal curve This video shows how to include multiple plots in the same figure. This shows you how to import some data and make a scatter plot. This shows you how to include multiple sets of data on the same plot. This shows you how to put two histograms on the same plot and gives you an example of how to subset data. "],
["sampling-distribution-for-proportions.html", "14 Sampling Distribution for Proportions 14.1 The binomial and the normal", " 14 Sampling Distribution for Proportions Learning objectives Understand how to find the standard error of the sampling distribution of a proportion. 14.1 The binomial and the normal A proportion is simply a scaled mean, so we treat proportions in a similar manner to means when trying to find its sampling distribution. So, for example, we imagine that everyone who supports Trump is a \\(y_i=1\\) and everyone who doesn’t is \\(y_i=0\\). The proportion of Trump supporters in our sample is therefore $ y_i / n = {y}$, which we usually denote p (just to keep things straight). As it is a statistics, the sampling distribution of a proportion is often approximated by the normal distribution with: \\[p \\sim Normal \\left(\\hat{\\pi}, \\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi})}{n}}\\right)\\] where \\(\\hat{\\pi}=p\\) (the observed proportion from the sample) and \\(n\\) is the sample size of our sample. Note that we do not have a separate parameter for the variance. This is because this formula derives from the binomial distribution, where both the mean and the variance are known so long as we know \\(\\pi\\) and \\(n\\). "],
["sampling-distributions-pt-2.html", "15 Sampling Distributions, Pt. 2 15.1 First, let’s review 15.2 How do we know what the shape of a sampling distribution is when the sample size is large? 15.3 Take a step back 15.4 How do we know what the mean of a sampling distribution is? 15.5 How do we know what the standard error is? 15.6 Some notes on notation 15.7 What’s the takeaway?", " 15 Sampling Distributions, Pt. 2 Learning Objectives 1)Be able to calculate the parameters of the sampling distribution for a sample mean in two circumstances: The population variance is known The population variance must be estimated from our data 2)Understand the notation for the center and spread of the population distribution, sampling distribution, and sample distribution 15.1 First, let’s review A sampling distribution is the theoretical distribution of sample statistics. We use sampling distributions to figure out how close a statistic calculated from a sample (e.g., a sample mean) is likely to be to the population parameter we’re concerned with. Each point on a sampling distribution represents a statistic calcualated from a different sample. 15.2 How do we know what the shape of a sampling distribution is when the sample size is large? Due to a rule known as the Central Limit Theorem, when the sample sizes are sufficiently large, the resulting sampling distribution will be normally distributed, regardless the population distribution. The videos below offer a good explanation of the Central Limit Theorem and how they relate to calculating sampling distributions. Seriously . watch them. No really. Don’t jump ahead. Don’t skip videos. Don’t fast forward. Watch these all the way through.If you find something confusing, watch it twice. This is probably the most important concept we will cover in the entire class. 15.3 Take a step back For this class you need to really (REALLY!) need understand this, so pay attention. When the sample size is large, the sampling statistic will be distributed normally no matter how the population is distributed. This fact comes from the Central Limit Theorem and is the basis for many of the statistical tests we will cover. In some other circumstances (that we will discuss in great detail later) the sampling distribution of a sample statistic may be a t-distribution or a binomial distribution (and there are a few others). Probably the most confusing thing about this class for many students is understanding what a sampling distribution is and figuring out which sampling distribution to use for different situations. So do yourself a favor. If you are still a little hazy about what a sampling distribution is, go and watch those videos again, come to office hours, or find some other resource. 15.4 How do we know what the mean of a sampling distribution is? So far, we’ve figured out what the shape of a sampling distribution is when the sample size is large, but we have not yet figured out what the center of the distribution is. When we are looking at the sampling distribution of sample means, then the mean of the sampling distribution is the mean of the population, \\(\\mu_\\bar{x} = \\mu\\). Pretty easy huh? But it’s a bit confusing to say. You may be thinking now, “But we don’t know the population mean, \\(\\mu\\)!” You are right. So we are going to estimate the mean of the sampling distribution as \\(\\hat{\\mu}_\\bar{x} = \\bar{x}\\). That is, our best guess for the mean of the sampling distribution is the mean of the sample distribution. Again, this is pretty easy but can be confusing to keep straight. 15.5 How do we know what the standard error is? The tricky part of finding a sampling distribution is calculating the standard error. If we know the population variance: To calculate the exact standard error for means, we use the population standard deviation. The formula to calculate the standard error for means is given by: \\[\\sigma_\\bar{x} = \\frac{\\sigma}{\\sqrt{n}}\\] If we don’t know the population variance AND the sample size is large: But what happens when we don’t know the population standard deviation, which, in all likelihood is the case? In that event, we estimate the population variance (and hence the variance of the sampling distribution) using the sample standard deviation. Our estimate of the population variance is: \\[\\hat{\\sigma}^2= \\text{s}^{2}\\] Therefore, our estimate of the standard error (the square root of the variance) of the sampling distribution is:. \\[\\hat{\\sigma}_\\bar{x}=\\frac{\\hat{\\sigma}}{\\sqrt{n}}=\\text{s}\\sqrt{\\frac{1}{n}}\\] You should be able to prove this to yourself using just the the two equations above. Try it for yourself, just to make sure you understand. Ask about it in class if you cannot quite get it. 15.6 Some notes on notation There are a lot of moving parts when it comes to samples, populations, and sampling distributions, so it is important to keep all the symbols straight. There are a few broad rules when it comes to assigning symbols. When there is a hat ($\\hat{\\mu}, \\hat{\\sigma}, \\hat{\\pi}$), that means that that variable is being estimated using a sample statistic. Alphabetic letters ($\\bar{x}$, $s$, $p$, $n$) are parameters of the sample distribution. Greek letters without hats or subscripts, ($\\mu, \\sigma, \\pi$) are parameters of the population distribution. Greek letters with subscripts ($\\mu_\\bar{x}, \\sigma_\\bar{x}$, etc.) are parameters of the sampling distribution. Below is a table of notations for samples, sampling distributions, and populations when we’re dealing with means. Distribution: Mean: Standard deviation: Estimated mean (using sample distribution): Estimated standard deviation (using sample distribution) Sample distribution \\(\\bar{x}\\) \\(s\\) N/A N/A Sampling distribution \\(\\mu_{\\bar{x}}\\) \\(\\sigma_\\bar{x}\\) (Called standard error) \\(\\hat{\\mu}_{\\bar{x}}\\) \\(\\hat{\\sigma}_{\\bar{x}}\\) Population distribution \\(\\mu\\) \\(\\sigma\\) \\(\\hat{\\mu}\\) \\(\\hat{\\sigma}\\) 15.7 What’s the takeaway? To calculate the shape of the sampling distribution, we look at the size of the sample, and statistics calculated from the sample distribution. The center of the population distribution is the center of the sampling distribution, and is estimated as the mean of the sample distribution, {x}. The standard deviation of the sampling distribution, known as the standard error, is calculated according to the formulas above. Sometimes we know the population standard deviation, we use that to calculate the standard error. \\[\\frac{\\sigma}{\\sqrt{n}}\\] Sometiems use the sample standard deviation to estimate the population standard deviation, when we don’t know the population variance. For the sampling distribution of sample means, the standard error is then: \\[\\text{s}\\sqrt{\\frac{1}{n}}\\] "],
["sampling-distributions-pt-1.html", "16 Sampling Distributions, Pt. 1 16.1 First, let’s review 16.2 What is a sampling distribution? 16.3 How is a sampling distribution different from the sample distribution or the population distribution? 16.4 What are the takeaways?", " 16 Sampling Distributions, Pt. 1 Learning Objectives: 1)Understand what a sampling distribution is, and how it is different from a sample distribution, or the population distribution. 16.1 First, let’s review Thus far, we’ve dealt primarily with two distributions: samples, and populations, with samples being a subset of the population. Often times, it can be difficult to know the exact parameters of a population, so we use samples to estimate. But samples are not perfect. Even a good random sample may have some error. That is, the statistics calculated from the sample may not exactly match up with the population parameters. So, for instance, the sample mean may not exactly match the population mean. We want to know how much error there will likely be in our sample statistics. To do this, we use the sampling distribution. The sampling distribution uses probability theory to describe how sample statistics will vary. For example, imagine we are running a study on the average household income in the United States. It would be infeasible to collect data for every single household, so we decide to do a survey. Let’s say we decide that our survey will have 2,000 households. We know that the sample mean might be slightly different from the actual population mean. But how different might it be? What is the probability that your sample will be off by 1,000? By 3,000? By 10,000? To answer this question, we use the sampling distribution, which tells us the expected results of a sample if we were to (theoretically) collect a sample of that size many, many times 16.2 What is a sampling distribution? A sampling distribution is how the sample means or sample proportion are distributed. In other words, if you were to draw large samples from a population repeatedly, and graph the resulting sample means, the sampling distribution is what that graph would look like. In the samples we’ve been dealing with so far, the standard deviation is a measure of how far individuals in a population tend to be from the population mean. It is a measure of how spread out individuals are in the population. In a sampling distribution, the standard error is a measure of how far a sample mean or proportion tends to be from the true population mean or proportion. (NOTE: Read this paragraph a few times and make sure you understand the difference.) To distinguish between samples, sampling distributions, and populations, it may help to think of this using college terms. Below are the college equivalents to these terms. Population: students at Wash U Population parameter: Average GPA at Wash U Sample: the students in one course Sample mean: the mean GPA in that one course Sampling distribution: the hypothetical distribution of mean course GPAs. Standard error: a measure of spread between a hypothetical mean course GPA and the school GPA Below is a short video describing a sampling distribution. (Don’t worry about the exact formulas yet. Just try and get the concept.) 16.3 How is a sampling distribution different from the sample distribution or the population distribution? The distribution of the sample (sometimes confusingly called the sample distribution) is the distribution of a single sample. In our example of household income, each point in the sample distribution represents one household that we surveyed. The sampling distribution shows the results of repeated samples, and each point in the sampling distribution represents one sample mean. The population distribution (if we could ever see it, which we cannot) shows every single possible data point in the population, and each data point represents one household. 16.4 What are the takeaways? A sampling distribution is the theoretical distribution of sample statistics. Each point on the sampling distribution represents a sample statistic. The standard error the sampling statistic is an indicator of how far a sample statistic is likely to be from the actual population parameter. "],
["working-with-the-t-distribution.html", "17 Working with the T-distribution 17.1 A bit about the t-distribution 17.2 Finding probabilities", " 17 Working with the T-distribution Learning objectives Get a general idea about the t-distribution. Learn how to find areas under the curve for the t-distribution using a table. 17.1 A bit about the t-distribution The t distribution is a theoretical probability distribution. It is symmetrical, bell-shaped, and similar to the standard normal curve. It differs from the standard normal curve, however, in that it has different parameter, called degrees of freedom, which changes its shape. This is usually labled \\(df\\) or \\(\\nu\\). Note that the smaller the df, the flatter the shape of the distribution, resulting in greater area in the tails of the distribution. The t distribution is useful for distributions with degrees of freedom less than 30. It is a more accurate way of describing distributions with relatively low degrees of freedom. As df approaches infinity, the t distribution becomes more like the normal distribution. 17.2 Finding probabilities For the time being, I only want you learn how know how to use a t-table, which you can find here. This is really best shown using an example. For a distribution with \\(n = 10\\), find the probability of \\(t \\ge 0.1\\). First, we need to determine the degrees of freedom (10-1 = 9). Once we have established the degrees of freedom as 9, we can use the t value to find the probability \\(P (t \\ge 0.1)\\). That is, we can now use our t-value in conjunction with our t table in order to find our probability. Once we have located how many degrees of freedom we have on our table (9, row 1, column 9) we can scan the columns for the right t-value to the first column, lableed \\(t_{.100}\\). The corresponding probability is 1.38. Note that this value corresponds to the probability to the right of the t-value. So if we want the area to the left, we will have to use some of the tricks shown on the normal distribution page. "],
["the-binomial-distribution.html", "18 The Binomial Distribution 18.1 The Binomial Distribution 18.2 Calculating probabilities using a table", " 18 The Binomial Distribution Learning Objectives Understand the characteristics of a binomial distribution and how it relates to a normal distribution. Understanding how to look up probabilities for the binomial distribution on a table. 18.1 The Binomial Distribution A binomial random variable is discrete, not continuous. This means that there are only two possible outcomes (examples: heads/tails coin toss, win/lose football game, support Obama/Not Obama). In general, there are certain requirements that must hold for the binomial distribution to be applicable. n repeated identical independent trials two outcomes (success/failure) P (success) + P(failure) must be equal to 1 The binomial function for calculating the probability distribution of the binomial for any probability p and number of trials n is as follows: \\[Pr (x = k) = {n \\choose k} p^k(1-p)^{n-k}\\] The mean and the variance of the binomial distribution are as follows: \\[\\mu= np\\] \\[\\sigma^2= np(1-p)\\] As useful as the binomial distribution is, it becomes more difficult to use as the values for n get larger. Lucky for us, when p = .5, the binomial distribution closely approximates a continuous density function which results in a smooth, symmetrical, bell-shaped curve called the standard normal distribution. In fact, the standard normal works for any value of p so long as n is large. See this video for some detailed examples of how to use the binomial distribution (don’t worry, you won’t be doing these calculations by hand!) 18.2 Calculating probabilities using a table Download this table and take a moment to look through it. With this table, we can find the probability of observing any possible number of successes for many possible values of \\(n\\) and \\(p\\). The far left column shows the size of the trial \\(n\\), and the second column shows the number of successes. The other columns show the probability of observing that number of successes for different values of \\(p\\). If the probability of a success is \\(p=0.20\\) and \\(n=8\\), the probability of observing two successes is 0.294. To see this, you go to the second page of the table linked above. Look near the top where the column labeled \\(n\\) says 8. Then look at the row where the column labeled \\(r\\) says 2. Follow this row along to the column labeled 0.20. That’s the answer. Try this yourself. If I have 16 trials and p=.55, what is the probability of observing 10 successes? The answer is 0.168. If this still doesn’t make sense to you, post a question on Facebook or go talk to one of the TAs in office hours. "],
["working-with-the-normal-distribution.html", "19 Working with the Normal Distribution 19.1 The Standard Normal Distribution 19.2 Z-scores 19.3 What’s the point of the z-score? 19.4 Examples", " 19 Working with the Normal Distribution Learning Objectives Understand the characteristics of a standard normal distribution, and how it relates to any normal distribution. Learn how to find the area under the normal curve for any interval. 19.1 The Standard Normal Distribution The standard normal distribution is a normal distribution with a mean of \\(\\mu=0\\) and a standard deviation of \\(\\sigma=1\\). Any normal distributions can be transformed to standard normal distributions using the following formula: \\[Z = \\frac{x-\\mu}{\\sigma}.\\] That is, if \\(x \\sim N(\\mu, \\sigma)\\) and we apply the formula above we will have a new variable that is distributed according to the standard normal distribution. If this seems a little abstract, read down to some of the examples below and then come back to this point and think about it. 19.2 Z-scores A z-score is what we obtain after we apply the above equation. So, for istance, if we are looking at a normal distribution with \\(\\mu = 2\\) and \\(\\sigma=2\\) we can calculate the z-score of the point \\(x=4\\) as \\[Z = \\frac{x-\\mu}{\\sigma}=\\frac{4-2}{2} = 1.\\] A z-score of 1 means that the the point \\(x=4\\) is on standard deviation above the mean. Likewise, a z-score of 2.4 would mean that \\(x\\) is 2.4 standard deviations above the mean. 19.3 What’s the point of the z-score? Once a z score has been calculated, it is very easy to calculate the probability of the area under curve using a table. If a dataset follows a normal distribution, then about 68% of the observations will fall within one standard deviation of the mean, about 95% of the observations will fall within two standard deviations of the mean, and about 99.7% of the observations will fall within 3 standard deviations of the mean.(Remember the empirical rule? If not, go back and review.) An explanation of an example problem is posted below in the examples section. But first watch this video. It is a bit long, but this is something you really need to be good at for the test. So it is totally worth your time. Click here to see the table we will use in class so you can follow along with the examples. (There are some more examples below). 19.4 Examples Below are some examples showing how to use the standard normal distribution and the z table in order to solve for probabilities.You should use the Agresti table when you try these example problems. This is what you will be using on the rest of the exams and homeworks for this course-so pay attention. Example 1: Finding the area to the left, when \\(x&lt;\\mu\\) For a normal distribution with \\(\\mu = 55\\) and \\(\\sigma = 3,\\) find the probability that an observation falls at or below the value 52.75. To find the z-score, plug in the mean and standard deviation into \\(Z = (x-\\mu)/\\sigma\\) The variables describing this normal distribution will be converted into a z score that can will used as part of a standard normal distribution \\[P(x \\le 52.75)=P(z \\le (52.75-55)/3))=P(z \\le -0.75)\\] Now, we use our calculated z score in conjunction with the z table above to find our probability. However, the table does not return negative values for z because the table is for the area to the right. So what should we do? The key is to remember that the standard normal distribution is symmetric around 0. This means that \\(P(z \\le -0.75)=P(z \\ge 0.75)\\). Notice the change from “less than” to “greater than.” If this doesn’t make sense, take a moment to draw a picture of the normal distribution. Since the distribution is symmetric, we can multiply the equation by -1 to get a value for the side of the table that we want. After doing so, our equation changes from \\(P(z \\le -0.75)\\) to \\(P(z \\ge 0.75)\\). Next we find the z score we just calculated on the corresponding z score table above. The first decimal place holds a 7; the 2nd decimal place holds a 5. When we locate the .7 (row 7 column 1) and then move over to the .05 column, we find the corresponding probability to be about 0.227. Remember that this probability refers to the area below the curve to the right of the z score we calculated (as shown by the graphic above the table). Lucky for us, this is exactly what the equation tells us to find \\((P(z\\ge0.75))\\). Our answer, therefore, for \\(P(x \\le 52.75)\\) = 0.227. Example 2: Finding the area to the right when \\(x \\le \\mu\\). For a normal distribution with \\(\\mu = 55\\) and \\(\\sigma = 3\\), find the probability that an observation falls at or above the value of 52.75. To find the z-score, plug in the mean and standard deviation into \\(Z = (x-\\mu)/\\sigma\\). The variables describing this normal distribution can be converted into a z score that can be used as part of a standard normal distribution \\[P(x \\ge 52.75) = P(z\\ge(52.75-55)/3)) = P(z \\ge -0.75)\\] We are looking for an area to the right, just like the table expects, but there are no negative values on our table, so what should we do? Again, we have to alter our equation so that we have a can use our z score in conjunction with the z table. The key is to realize that \\(P(z\\ge-0.75) = 1-P(z\\le-0.75)\\). This is true because the total area under the normal curve always sums to 1. So, for any point x, the area to the right is always one minus the area to the left. Again, if this doesn’t make sense, draw yourself a picture. So to complete this problem, all we need to do is find \\(P(z\\le -0.75)\\). But this is exactly what we found in the last problem! So we find the area to the right of positive 0.75 just like before, which is 0.227. The probability value we find is the same as the above value: 0.227. However, note that this value designates the probability to the right of the z score-and we want probability to the left of the z score. In this case, we take (1-) the probability to the right of the z score to get the probability to the left of the z score. So 1 - 0.227 gives us our answer: 0.773. Example 3: Finding the point given an area For a normal distribution with \\(\\mu = 55\\) and \\(\\sigma = 3\\), I want to find the point where the area under the curve to the right is equal to 0.0985. Don’t panic. The first thing to realize is that this is basically just an algebra problem. You are pretty good at Algebra, right? First, set up the equation based on the formulas below. \\[P(Z&gt;\\frac{x-\\mu}{\\sigma})=0.0985 \\rightarrow P(Z&gt;\\frac{x-55}{3})=0.0985\\] So this looks bad. It looks like there is only one equation above, but there are two unknowns, \\(Z\\) and \\(x\\). The trick is to realize that we can use the table to find the correct value of \\(Z\\). For these kinds of problems, we just need to look around the Z-table until we find a value that is as close as possible to 0.0985. Somtimes, you will have to guesstimate when the exact number you are looking for isn’t there. In this case, the number is right there in the row that starts with 1.2 in the column labled .09. This means that the z-value we plug into the equation above is 1.29! \\[P(1.29&gt;\\frac{x-55}{3})=0.0985\\] In fact, now that we have found the value of \\(z\\), we don’t even need most of this formula. All we need to do is solve the equation below for \\(x\\). \\[1.29 =\\frac{x-55}{3}\\] So, after a few easy steps with algebra, we see that \\[x=58.87.\\] "],
["constructing-confidence-intervals-using-proportions.html", "20 Constructing Confidence Intervals Using Proportions 20.1 What is a proportion? 20.2 Confidence Intervals With Proportions 20.3 Example 20.4 Having trouble with the notation?", " 20 Constructing Confidence Intervals Using Proportions Learning Objectives Further understand proportions and how they differ from population means Understand how to construct confidence intervals for proportions. A little clarification on the notation for proportions. 20.1 What is a proportion? Proportions are significantly different from population means. Proportions are for dichotomous variables: variables that are characterized by only two values. So supposing we have some variable called Obama Approval Rating. This is a dichotomous variable because there are really only two values we are interested in: the percentage that “approve” and the percentage that did not respond that they “approved.” Contrast this with population means that take the form of quantitative variables. Quantitative variables are what we have predominantly dealing with in the course so far. These are characterized by values that differ for each individual respondent. That is, our data for each person might be many different numbers. Height, for example, is a continuous quantitative variable because each data point has a different value (Note how this differs from Obama Approval Rating where each data point can only take one of two values). 20.2 Confidence Intervals With Proportions Statistics problems that use proportions use most of the same equations as the problems for population means. But to calculate the standard error for proportions, we use the following formula: \\[\\hat{\\sigma}_{\\hat{\\pi}}=\\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi}) }{n}}\\] For a refresher on why this might be, go back and review the information on the sampling distribution of a proportion. Our estimator (\\(\\hat{\\pi}\\)} for the population proportion (\\(\\pi\\)) is the sample proportion \\(p\\). The standard equation for writing a confidence interval for a proportion is as follows: \\[\\pi \\pm Z \\times \\sigma_{\\pi}\\] The confidence that we can have for any proportion varies with size of the sample and the degree to which the two values for our dichotomous variable match. As \\(n\\) increases, the confidence we have in our statistic also increases. Note also, that as the value for our proportion approaches 50% (or .50), the less confidence we have in our statistic. If that seems a little weird, go back and look at the equations above and try a few different numbers for p, and see what you get. 20.3 Example Suppose that the point estimate for the proportion of Americans who approved George W. Bush as president in 2008 is 25% where n = 5000. Construct a 95% confidence interval for this proportion. First, write out the equation for the confidence interval for a proportion: \\[\\pi \\pm z \\times \\sigma_{\\pi}\\] We already have the point estimate for this proportion (p=0.25). We still have to calculate the appropriate Z value and the standard error. To get the Z value, we need to calculate for the observation that corresponds to a 95% confidence interval. In general, to find the observation that corresponds to some confidence interval we take (1-confidence coefficient)/2. In this case: (1-.95)/2 = .025. To find the Z value that corresponds, we simply look this probability up in a Z table. We find .025 in the column that reads .06 and the row that reads 1.9. Therefore, we have found our Z value to be 1.96. Next we need to calculate our standard error using the following formula: \\[\\hat{\\sigma}_{\\hat{\\pi}}=\\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi}) }{n}}\\] \\[\\hat{\\sigma}_{\\hat{\\pi}}=\\sqrt{\\frac{.25(1-.25)}{ 5000}} = .0061\\] We are now ready to construct our confidence interval by plugging in our calculated values into the equation above. \\[.25 \\pm 1.96\\times .0061\\] \\[.25 \\pm .012\\] Here is another example of how to calculate a confidence interval for a proportion you might find instructive: 20.4 Having trouble with the notation? Below is a table of notations for samples, sampling distributions, and populations when we’re dealing with proportions. Distribution: Mean: Standard deviation: Estimated mean (using sample distribution): Estimated standard deviation (using sample distribution) Sample distribution \\(\\bar{x}\\) \\(s\\) N/A N/A Sampling distribution \\(\\mu_{\\bar{x}}\\) \\(\\sigma_\\bar{x}\\) (Called standard error) \\(\\hat{\\mu}_{\\bar{x}}\\) \\(\\hat{\\sigma}_{\\bar{x}}\\) Population distribution \\(\\mu\\) \\(\\sigma\\) \\(\\hat{\\mu}\\) \\(\\hat{\\sigma}\\) "],
["confidence-intervals-with-the-z-and-t-distributions.html", "21 Confidence Intervals with the z and t-distributions 21.1 So, first let’s review … 21.2 What is a confidence interval and how d o we calculate it? 21.3 Knowing when to use the formula (and when not to) 21.4 Should you be using the t or the z? 21.5 Summary:", " 21 Confidence Intervals with the z and t-distributions Learning objectives: Understand the concept of a confidence interval and be able to construct one for a mean Understand when (for what kinds of data) to use the standard error formula \\[\\frac{s}{\\sqrt{n}}\\] Know when to use the t-distribution and when to use the z-distribution for constructing intervals 21.1 So, first let’s review … It’s important to remember the difference between a sample distribution and a sampling distribution. Remember that a sample data distribution is the distribution of the data points within a single sample. A sampling distribution is the probability distribution a statistic can take. Keep in mind also that, by the Central Limit Theorem, the sampling distribution of the sample mean \\((\\bar{y})\\) is approximately normal regardless of the shape of the original distribution of the variable. For a solid review of these key concepts, check out the previous pages here and here. 21.2 What is a confidence interval and how d o we calculate it? Learning Objective 1: Understand the concept of a confidence interval and be able to construct a confidence interval for a mean Roughly speaking, a confidence interval is range of numbers within which we believe the true population parameter to fall. Here, we will focus on how to calculate a confidence interval for a population mean. Let’s think about this in terms of z-scores. When we calculate a z-score, we want to look at how many standard deviations away from the mean our data point falls. A confidence interval relies on a similar principle. We want to estimate how many standard errors our sample mean falls from the true population mean on the sampling distribution. We use the normal distribution because the sampling distribution for sample mean is always normal by the Central Limit Theorem. Recall that when we calculated z-scores, each one had a probability associated with it that told us the probability of getting a value at or above that given value? (For example, when you find that the z-score for a given observation equals 1.96, you know that the probability of finding a value at or above that value is .025 and below is 1-.025 = .975). So, using our tables , we know that 95% of a normal distribution falls within 1.96 standard deviations of the mean. Since the mean of the sampling distribution (\\(\\mu_{\\bar{y}}\\)) is the population mean \\((\\mu\\)), we can say that, with probability .95, the sample mean \\(\\bar{x}\\) will fall within 1.96 standard errors of the population mean. NOTE: We use the standard error because we are looking at the sampling distribution and not the sample data distribution. The following graph illustrates (x is the same as y here). Since what we are trying to do is establish in what interval the true population parameter falls, we add and subtract from the sample mean, a multiple of its standard error, or the z-score for the probability we want. Why? Because, as stated above, we know that for the normal distribution that specific probability and that specific z-score always go together. For sample means the formula is: \\[\\bar{Y} \\pm Z (\\frac{S}{\\sqrt{n}})\\] We use \\(S\\) rather than \\(\\sigma\\) because we we are estimating the population standard deviation. If we know \\(\\sigma\\), we can use it in our calculations instead. In reality, we do not know \\(\\sigma\\), but this sometimes is asked about on homeworks or exams. 21.3 Knowing when to use the formula (and when not to) Learning objective #2: knowing when to use the above formula We want you to be sure to understand when to use the formula. \\[\\bar{x} \\pm z (\\frac{s}{\\sqrt{n}})\\] Often in statistics classes, people get incorrect answers because they were confused about which formula to use. We use the above formula only when we are calculating a confidence interval for a mean, not for a proportion. We will need a different formula. Suffice it to say for now that we use the above formula only with data for which we can calculate a numerical mean. What kind of data is that? Quantitative, data (i.e., not nominal). For example, a data set measuring human height is a quantitative data set. Why? Because height is a variable that is naturally measured numerically. We can construct a confidence interval for the mean height in a sample using the above formula for this reason. To understand this concept a bit better, think about a non-quantitative variable like eye color. You cannot measure the color of someone’s eyes numerically, therefore a data set examining whether or not a given sample has green eyes or not is not quantitative data. And if it is not quantitative data, we cannot use the this formula. 21.4 Should you be using the t or the z? Learning objective #3: Understanding when to use the z-distribution v. the t-distribution Keep in mind: we are able to use the normal distribution to construct our confidence interval because of the Central Limit Theorem. When \\(n \\geq 30\\), the sampling distribution of the sample mean becomes normal. If \\(n \\leq 30\\), the skewness of the population could influence the shape of the sampling distribution and make it not normal. But what do we do if we want to calculate confidence interval for a sample size where \\(n\\leq 30\\)? We use the t-distribution, but only if we feel it is appropriate to assume that the population distribution itself is normal (or close to normal). Constructing confidence intervals with t-distribution is the same as using the z-distribution, except it replaces the z-score with a t-score. Recall the above formula for calculating the confidence interval for a mean. Notice again that we used the sample standard deviation, \\(s\\), instead of the true population standard deviation, \\(\\sigma\\), in our calculation of the standard error. This estimation of \\(\\sigma\\) introduces extra error, and this extra error can be pretty big when \\(n\\leq 30\\). Because $s $is a poor estimator of \\(\\sigma\\)with a small sample size, we will not assume that the sample distribution is normal. Instead, we will use a t-distribution, which is designed to give us a better interval estimate of the mean when we have a small sample size. In fact, the t-distribution provides us with a different sampling distribution for each small sample size. Here are the fundamental principles for using the t-distribution for confidence intervals: You cannot use the t-distribution unless you assume that the population distribution of the variable is normally distributed. The t-distribution, like the z-distribution, is bell-shaped and symmetric about a mean of 0. The t-distribution incorporates the fact that for smaller sample sizes the distribution will be more spread out using something called degrees of freedom. For confidence intervals, the degrees of freedom will allways be \\(df = n-1\\), or one less than the sample size. For every change in degrees of freedom, the t-distribution changes. The larger the sample size (n), the closer the t-distribution mimics the z-distribution in shape. We construct a confidence interval for a small sample size in the same way as we do for a large sample, except we use the t-distribution instead of the z-distribution. The formula is: \\[\\bar{x} \\pm t (\\frac{s}{\\sqrt{n}})\\] This video by Khan does a particularly good job of explaining why we need the t-distribution. If you want the explanation watch the whole thing. If you just want to watch him calculate an example, start watching around 4:02. 21.5 Summary: The formula for calculating a confidence interval for a mean is : \\(\\bar{x} \\pm z (\\frac{s}{\\sqrt{n}})\\) We can only use the formula \\(\\bar{x} \\pm z (\\frac{s}{\\sqrt{n}})\\) for quantitative data We use the t-distribution for sample sizes of normal data with \\(n \\leq 30\\). "],
["estimating-population-means-using-confidence-intervals.html", "22 Estimating population means using confidence intervals 22.1 Statistical inference is how we draw conclusions from the data. 22.2 We make statistical inferences by using sample statistics to estimate population parameters. 22.3 The method we use to make statistical inferences using sample statistics is called estimation. 22.4 Constructing confidence intervals to make statistical inferences. 22.5 Remember that we already learned about the standard error …. 22.6 Remember that we already learned about the z-score …. 22.7 Example. 22.8 Conclusion.", " 22 Estimating population means using confidence intervals Learning Objectives Understand what statistical inference is Understand how we make statistical inferences using sample statistics to estimate population parameters Understand what kind of estimate a confidence interval is Understand how to do a simple confidence interval calculation 22.1 Statistical inference is how we draw conclusions from the data. Statistical inference is the process that we use to draw conclusions from a data. A set of data is a sample from our population. The sample is a subset of the population. Inference involves using statistics we calculate from the sample to make and informed guess about population. When we do statistical inference we are interested in drawing conclusions from a set of data (sample) so that we can estimate population parameters. (Population parameters are the characteristics of the population in which we are interested.) Parameters of the population include things like the mean, standard deviation, and variance. Statistical inference is the process by which we use sample statistics to make inferences about population parameters. Figure 1 and Table 1 describe the relationship between sample statistics and population parameters. 22.2 We make statistical inferences by using sample statistics to estimate population parameters. Using sample statistics we can make estimates about population parameters. There are two types of estimates of parameters: point estimates and interval estimates. A point estimate is a single number that is the best guess for the parameter. An interval estimate is an interval of numbers around the point estimate, within which the parameter value is believed to fall. 22.3 The method we use to make statistical inferences using sample statistics is called estimation. The term estimator refers to a particular type of statistic for estimating a parameter. This is conceptual. The term estimate is a noun that refers to the value of that particular statistic. For example, the sample mean is an estimator of a population mean. If the sample mean of a particular sample 75 then the value 75 is the estimate for the population mean. Just as there are multiple possible sample statistics, there are are many possible estimators. If we are interested, for example, in the population’s mean, then we could use the mean, the median, or the mode of the sample as our estimator. How do statisticians decide which estimator is best? Good estimators usually have two qualities: they are unbiased and efficient. An unbiased estimator has its sampling distribution “centered” around the parameter; in other words, the expected value of the statistic is identical to the population parameter. An efficient estimator has a relatively small standard error, meaning that the variance of the sampling distribution is small. Unbiased and efficient estimators give us accurate and precise estimates of population parameters. The point estimators we teach you in this class have been shown to be the most efficient unbiased estimators possible. 22.4 Constructing confidence intervals to make statistical inferences. A truly informative statistical inference, however, should provide not only a point estimate but should also indicate how confident we can be that the estimate is correct. So, rather than a single value, we often prefer to use a range of values. This is what is known as an interval estimate: an interval of numbers (usually centered around some point estimate) within which the parameter value is believed to fall. Another name for interval estimates is confidence intervals, because they contain the parameter with a certain degree of confidence. Confidence intervals have the following form: Point estimate \\(\\pm\\) Margin of Error. Specifically, we can calculate confidene intervals for the sample mean …. Under certain circumstances, confidence intervals for means take the following formula: \\[\\bar{y} \\pm z (\\frac{s}{\\sqrt{n}}) \\] Where, \\(\\bar{y}\\) = sample mean \\(z\\) = z-score corresponding to confidence level \\(s\\) = sample standard deviation \\(n\\) = sample size The point estimate of the population mean \\(\\mu\\) is the sample mean \\(\\bar{y}\\). For large random samples, by the Central Limit Theorem, the sampling distribution of is approximately normal. So, for large samples, we can find a margin of error by multiplying a z-score from the normal distribution by the standard error. 22.5 Remember that we already learned about the standard error …. The standard error of the sample mean is given by the following equation: \\[\\sigma_\\bar{y} = \\frac{\\sigma}{\\sqrt{n}} \\] In the above equation, \\(\\sigma\\) is the population standard deviation. But like \\(\\mu\\), \\(\\sigma\\) is an unknown parameter. In practice, we estimate \\(\\sigma\\) using the sample standard deviation \\(s\\). So, in practice, confidence intervals use the estimated standard error. This is how we end up with the equation for confidence intervals: \\[\\hat{\\sigma}_\\bar{y} = \\frac{s}{\\sqrt{n}} \\] 22.6 Remember that we already learned about the z-score …. When we are calculating a confidence interval for a mean, our point estimate is our sample mean. Our standard error is calculated by dividing the sample standard deviation by the square root of the sample size. We have yet to understand where the z-score comes from. The z-score corresponds to the confidence level we are interested in. We choose a confidence level before we go about constructing our confidence interval. We try to choose confidence levels that are close to (but not exactly) 100%. The higher the confidence level, the more confident we can be that our unknown parameter falls within the interval estimate. The z-score indicates what our confidence interval is. Recall that 95% of the normal distribution falls within 1.96 standard deviations of the mean. We use the same logic to calculate the z-score for our confidence intervals. If we are interested in a 95% confidence interval, then our z-score is 1.96; if we are interested in a 99% confidence level, then our z-score is 2.575. In general, in order to find the z-score that corresponds to our confidence level, we take \\[(1-\\mbox{confidence coefficient)}/2.\\] The z-score that corresponds with the resulting probability is the z-score that we use when we construct our confidence interval. NOTE: The confidence coefficient is often denoted by the Greek letter \\(\\alpha\\) 22.7 Example. Let’s do a quick example. Let’s suppose we have a sample in which the sample mean is 10 and the standard error is \\(\\frac{s}{\\sqrt{n}}=2\\). We want to calculate a 95% confidence interval. We use the following formula: \\[\\bar{y}\\pm z(\\frac{s}{\\sqrt{n}}) = 10 \\pm z(2) \\] In this case, the z-score that corresponds to a 95% confidence interval is \\(z=1.96\\). \\[10 \\pm 1.96(2) = 10 \\pm 3.92\\] Note that the formula above is only representative of our confidence interval. It shows our original point estimate and margin error. Our confidence interval must be a range of numbers In this case, it is: [6.08, 13.92]. What does this mean? It means, (kinf of) that according to our calculations, we can say with 95% confidence that our population mean falls between 6.08 and 13.92. Technically, however, that interpretation is not quite right. The 95% confidence level means that if we took many, many samples and calculated many, many confidence intervals according the procedures above, 95% of the time the true population parameter would fall within those many, many 95% confidence intervals. We actually don’t know the probabilyt that \\(\\mu\\) falls within the particualr interval [6.08, 13.92]. This is a bit confusing to think about, and most of the time you can think of it using the sentence in the last paragraph. But, keep in mind that the actual interpretation is a bit muddier. Here is a nice video that goes through another example and explains the relationship between the confidence interval and the normal distribution: 22.8 Conclusion. In this section we learned that we make statistical inferences to make conclusions from our data. Statistical inference requires that we estimate population parameters using sample statistics. One way we can make a statistical inference is by constructing confidence intervals. Confidence intervals give us an interval of numbers around the point estimate within which the parameter value is believed to fall. Although confidence intervals can be constructed for means or proportions, in this article, we learned that how to construct a confidence interval for a population mean (parameter) using the sample mean and sample standard deviation (sample statistics). "],
["choosing-n-for-ci-of-a-proportion.html", "23 Choosing N for CI of a Proportion 23.1 Choosing “N” for a confidence interval of a proportion 23.2 Manipulating the formula for a confidence interval for a proportion 23.3 Take-away message", " 23 Choosing N for CI of a Proportion Learning objectives Understand the variables that go into choosing “N” for a confidence interval of a proportion Undertand the formula for finding “N” for a confidence interval of a proportion Understand how the variables within the formula for the confidence interval of a proportion work with one another 23.1 Choosing “N” for a confidence interval of a proportion You need three pieces of information to choose your sample size. 1) Make a guess about the value of \\(\\pi\\) In some cases, we may have prior information (e.g., previously conducted studies) that inform our estimate of the true value of the populaiton proportion \\(\\pi\\). In other cases, we can “guess” what the standard error is. If it is not known, one can plug in a value of 0.5 for \\(\\hat{\\pi}\\). This works because a value of \\(0.5\\times(1-0.5)\\) provides the largest possible standard error (since a proportion can only be between 0 and 1). This largest standard error means that it can be used as a conservative estimate. By “conservative” we mean we have maximized our chances that the true popultation proportion will be inside our confidence interval. 2) Z-score The Z-score might be manifested by providing a “confidence-level.” This confidence level must then be converted to a corresponding Z-score. For instance, if you want to make sure you have a 95% confidence interval, you need a Z-score of 1.96. 3) Maximum Error (E) This value corresponds to the furthest away a value can be from the mean while still being inside of the confidence interval. This is often referred to as our margin of error. For example, if we want to be able to estimate a population proportion within \\(\\pm 3\\) percentage points, our maximum error is .03. 23.2 Manipulating the formula for a confidence interval for a proportion Let’s go through the steps for isolating “n” The formula for CI for a proportion: \\[CI= \\hat{\\pi} \\pm Z \\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi})}{n}}\\] We can re-write this as: \\[ CI= \\hat{\\pi} +/- E\\] That is, $-E $is equal to the lower bound and \\(\\hat{\\pi}+E\\) is equal to the upper bound of the confidence interval. Therefore, we can think of this as \\[E= Z\\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi} ^)}{n}}\\] This last step might be confusing. What I did was take the upper bound of the confidence interval and subtract from it \\(\\hat{\\pi}\\). This gives the value for E. That way, you can change “CI” to “E” and get rid of p on the other side of the equation. NOTE: It is important to remember that \\(\\hat{p}\\) may not be provided in the problem. If this is the case, one may use the value 0.5 and substitute it in. Using some algebra on the equation above, we get: \\[\\sqrt{n}=Z\\frac{\\sqrt{\\hat{\\pi}(1-\\hat{\\pi})}}{E}\\] and \\[n = \\frac{Z^2(\\hat{\\pi}(1-\\hat{\\pi}))}{E^2}\\]. If your answer for “N” is not a whole number, round up. That is, even if the found “N” is 12.03, round up to 13. The reason for this, is that you want to be conservative when creating a confidence interval, so that we can be confident of our range. This video provides an example. 23.3 Take-away message Learning Objective 3: Understand how the variables within the formula for the confidence interval of a proportion work with one another Hopefully you now understand how one finds “N” for the confidence interval of a proportion. The formula for such a calculation is: \\[n = \\frac{Z^2(\\hat{\\pi}(1-\\hat{\\pi}))}{E^2}\\]. You get the Z value from the confidence interval. You get \\(\\hat{\\pi}\\) either from a prior study’s data or as a conservtive guess of 0.5. The maximum error will usually be provided to you. Once you get all of that information, you just put it into the formula, and have “N”! Hopefully this section has given you a better sense for how the variables within the confidence interval of a proportion work with one another. "],
["sample-size-for-ci-for-the-mean.html", "24 Sample Size for CI for the Mean 24.1 Here’s the setup … 24.2 Now let’s go through the steps for isolating “n”…. 24.3 Let’s look at that again … 24.4 Summary", " 24 Sample Size for CI for the Mean Learning Objectives Understanding the information you need before calculating the sample size. Manipulating the formula for a confidence interval for the mean to find the right sample size 24.1 Here’s the setup … When given certain information, it is possible to figure out how many individuals we will need for our sample to make a specific confidence interval for an unknown population mean. There are 4 main pieces of necessary information A guess about the standard deviation (\\(\\sigma\\)), usually taken from a previous survye or another sample from a similar population. It is important to note that if you have no information about the standard deviation, there is no default choice. This is contrary to how it works with proportion. If you don’t have any other information, you are basically going to have to use your prior beliefs (i.e., guesstimate). Z-Score The Z-score might be manifested by providing a “confidence-level.” This confidence level must then be converted to a corresponding Z-score. (e.g., 95% confidence correlates with a Z-score of 1.96) Maximum Error (E) This value corresponds to the furthest away a value can be from the mean while still being inside of the confidence interval. For example, if a CI has a mean of 100 and an interval of (90,110) then “E” is equal to 10. 24.2 Now let’s go through the steps for isolating “n”…. Recall that the formula for CI for the Mean is: \\[CI= \\mu \\pm Z(\\frac{\\sigma}{\\sqrt{n}})\\] We are going to re-write this as: \\[ CI=\\mu \\pm E \\] That is, \\(\\mu-E\\) is equal to the lower bound and $+E $is equal to the upper bound of the confidence interval. Combining the two equations above, we see that \\(E= Z(\\frac{\\sigma}{\\sqrt{n}})\\). This last step might be confusing. What I did was take the upper bound of the Confidence Interval and subtract from it \\(\\mu\\). This gives the value for E. That way, you can change “CI” to “E” and get rid of \\(\\mu\\) on the other side of the equation. Now we can isolate n on one side of the equal sign using basic algebra. \\[ \\sqrt{n}=\\frac{Z \\sigma }{E}\\] Therefore, to find the number of individuals in a sample, one should use the formula: \\[n= \\left(\\frac{Z\\sigma}{E}\\right)^2\\] NOTE: If your find N is not a whole number, round up. That is, even if the found “N” is 12.03, round up to 13. The reason for this, is that you want to be conservative when choosing a sample size. 24.3 Let’s look at that again … 24.4 Summary In this section, you should have become more familiar with confidence intervals for the mean. We have taken you through finding the necessary information to find n. We have taken you through manipulating the formula to find n. However, this section should have also given you a sense of how the formula works in general. The goal is to have a true understanding of the function of each variable within a Confidence interval. "],
["working-with-the-normal-and-t-distribution-in-r.html", "25 Working with the normal and t distribution in R", " 25 Working with the normal and t distribution in R This video shows you how to work with the normal distribution in R: This video shows you how to work with the t-distribution in R: "],
["binomial-testing.html", "26 Binomial Testing 26.1 Small-Sample Test for a Proportion 26.2 Learning Objectives 26.3 Binomial Distributions 26.4 Null Hypothesis 26.5 P-values", " 26 Binomial Testing 26.1 Small-Sample Test for a Proportion 26.2 Learning Objectives Review binomial distributions Understand the null hypothesis Use the binomial distribution table to determine probabilities Understand a p value and its meaning at a given value 26.3 Binomial Distributions As we discussed a few weeks ago, a binomial variable is discrete, which means there are two possible outcomes (the Cardinals will win/lose a game, the president will veto/not veto a bill). We also know that the binomial distribution is perfectly symmetric only when \\(\\pi\\) = 0.50. This makes sense, because when considering an event where there are two possible outcomes (x and y), we would expect that the majority of the time, the outcome of half of the trials would be outcome x, and the outcome of the other half of the trials would be outcome y. 26.4 Null Hypothesis The null hypothesis is the probability of an event that we would expect as statisticians assuming that the outcomes in each trial happened by chance and that the world worked according to the laws of statistics. It is what a skeptic would say about the likelihood of an event. For example, let’s say that I flip a coin two times. Here is what could happen: Flip 1 Flip 2 Probability of this occurring T T 1/4= 0.25 T H 1/2= 0.5 (assuming we do not care about the order of the results) H T H H 1/4=0.25 In a world run by statistics, we can predict that 50% of the time, when we flip a coin twice, we will end up with one heads flip and one tails flip. This represents the null hypothesis: \\(\\pi_0\\) = 0.5 Suppose we flip the coin twice and get two tails. We want to find out what this result means assuming that the null hypothesis is true. The way we can do this is by determining the probability of flipping two tails in a row (assuming that \\(\\pi=.5\\)), then determining if that probability is “small” (assuming that the null hypothesis is correct). If the probability of two coin flips is very low under this assumption, we can “reject” the null hypothesis. IMPORTANT NOTE: When conducting a hypothesis test, you can either reject the null or not reject the null. You can never accept the null. That is because in science, we do not seek to prove things are true, but to disprove things that are not true. ## Binomial Distribution Table The table can be found here. Please review this table and make sure you understand how to read it for class on Monday. If you are confused, refer back to this article. 26.5 P-values A p-value is, the probability that the test statistic equals the observed value or a value even more extreme in the direction predicted by \\(\\pi_a\\), calculated by presuming \\(\\pi_0\\) is true. The p-value can be found on your binomial distribution table. It represents the probability of finding a certain set of outcomes given the null hypothesis. With our coin flipping example, I can find out using the binomial distribution table that for the outcome TT, p=.250. That means 25% of the time; I can expect to see these results. The p-value is a measure of surprise. It tells us how surprised we should be about a given outcome or something more extreme. In one article, R.A Fisher, statistician extraordinaire, decided that if the p-value is .05 or less, than this is surprising enough to reject the null hypothesis. If you are good enough at math, apparently you can do things like that. What this means is that if the probability of an occurrence OR SOMETHING MORE EXTREME is less than .05, our results are so surprising that we reject the null. Otherwise we do not reject the null. Let’s go back to that coin flip example. If our null hypothesis is that \\(\\pi=.5\\) and our research hypothesis is that it is an unfair coin biased towards Tails (\\(\\pi_a &gt; .5\\)). Let’s say we flip the coin twice and observe two tails. We see that p=.250 for two coin flips with the outcome TT. Because .250&gt;.05, we do not reject the null. Conceptually, this means that getting the result TT fits within our knowledge of statistics; that it is not sufficiently unlikely for you to flip a coin two times and get the results TT. "],
["large-n-hypothesis-testing-for-proportions.html", "27 Large-n hypothesis testing for proportions 27.1 Qualitative data 27.2 Learning Objectives 27.3 Science is about testing claims with data 27.4 But you will need to construct a hypothesis test… 27.5 This all sounds a bit confusing, so let’s break this down… 27.6 Watch both of these videos", " 27 Large-n hypothesis testing for proportions 27.1 Qualitative data 27.2 Learning Objectives Understand conceptually why we use hypothesis tests Understand the five parts of a hypothesis test A couple of very helpful videos videos 27.3 Science is about testing claims with data Let’s say that you are arguing with your friends over dinner, and they claim that WashU students are more conservaitve than the national average. Since everyone you know seems to be an Obama fan, you don’t believe that is true. You are a determined skeptic. One way you could solve this problem would be to argue really loudly (whoever is most obnoxious wins). Another way to win might be to tell your friend about some anecdotes and examples (tell them about your liberal classmates). Neither of these represent good ways to really test competing claims. But f you think about it, this is how most political arguments seem to be resolved (at least on cable TV). But a better way is to try to resolve this conflict using data. Because you can use Google, you know that 20.7% of College Freshman nationwide identified themselves as Conservaitve. http://heri.ucla.edu/PDFs/pubs/TFS/Norms/Monographs/TheAmericanFreshman2011.pdf Imagine you take a simple random sample of 50 WashU Freshman. 7 of them identify themselves as conservaves, 14 of them identify themselves as liberal, and the rest sa they are “middle of the road.” There are twice as many liberals as conservaitves in your sample! Based on this sample evidence, can you determine whether or not your friend is right? Yes. Yes, you can. 27.4 But you will need to construct a hypothesis test… In a hypothesis test, we have an assumption that the population parameter takes a particular value, called the null hypothesis. This usually corresponds to the claim you are trying to disprove. In the above example, the null hypothesis (the view of the determined skeptic) would be that the proportion of students at WashU is 20.7% or less (\\(H_0: \\pi_0 \\le 0.207\\)). The alternative hypothesis, your friend’s theory, is that the proportion of students who are conservative is more than the national average (\\(H_a: \\mu_a&gt;0.207\\)). A hypothesis test analyzes the evidence against the null hypothesis. It tells us if we would be likely to get our sample value just because of normal sampling variability if the null hypothesis were true. We judge how far the estimate falls from the null hypothesis’ parameter value, and then look at the probability of getting a sample value this far away from the null hypothesis value or further. 27.5 This all sounds a bit confusing, so let’s break this down… Learning objective 2: Understand the five parts of a hypothesis test The five parts of a hypothesis test are: assumptions, hypotheses, test statistic, p-value, and conclusion. 1) Assumptions To perform a hypothesis test you must assume: The data is obtained through random sampling We have qualitative data (things that are coded zero or one) As a rule of thumb, we want a sample size such that \\[ n \\ge \\frac{10}{min(\\pi_0, 1-\\pi_0)}\\] min() means the minimum of the two numbers The 10 in that equation is kind of arbitrary (it serves as a rule of thumb) If the sample size is this large, you can assume that there is a normal sampling distribution for the sample proportion. This gives us the leverage we need to make some calculations. 2) State hypotheses The null hypothesis is going to be that the true population proportion, \\(\\pi\\), is equal to the null hypothesis value, \\(\\pi_0\\). In this case \\(\\pi_0\\le0.207\\). The alternative hypothesis is that \\(\\pi_a\\) is greater than \\(\\pi_a &gt; 0.207\\). 3) Calculate test statistic For large samples, we use a test statistic to determine how many standard errors (we use standard errors and not standard deviations because we are working with the sampling distribution) our observed sample proportion falls from the null hypothesis. Here, we will denote our test statistic \\(TS\\) Using the information above, we know that \\(\\hat{\\pi} = 0.14\\). \\[TS=\\frac{\\hat{\\pi} - \\pi_0}{se_0}\\] Intuitively, this means that the farther $ falls from \\(\\pi_0\\) on the sampling distribution, the bigger our test statistic. However, it is important to note that we have to calculate our test statistic under the assumption that the null hypothesis is true. So, we are going to use the following formula for proportions \\[\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}} \\] In words, that means we are going to calcualte the standard error for the sampling distribution under the assumption that \\(\\pi=\\pi_0\\). In our case this means that \\[\\sqrt{\\frac{0.207(1-0.207)}{50}}=0.057\\] That means that \\[TS=\\frac{0.14 - 0.207}{0.057}=-1.169\\] 4) Calculate the p-value A p-value is the probabiliyt of observing your sample statistic, or something more extreme, assuming that the null hypothesis is true. Remember that in a sampling distribution, each value of \\(\\hat{\\pi}\\) has a probability associated with it. Values that fall very far away from \\(\\pi_0\\) have a very low probability of occurring if the null hypothesis is true. To determine that exact probability, we use the z-table! We simply go to the z-table and look up what probability is associated with our TS value. The below picture illustrates. In the sampling distribution, sample proportions that are closest to the true population proportion have a higher probability of occurring. Sample proportions that are way out in the tails are less likely to occur. NOTE: Some hypothesis tests are two-sided and others are one-sided. Most z-tables only provide you with a one-sided probability. So if you are performing a two-sided test, you will need to multiply your p-value by two. This multiplication is basically a way to account for not only the probability of obtaining a y.bar as large or larger than the one you obtained, but also a way to account for the probability of obtaining a value as or more extreme than the one obtained in either direction. In this, case we can look up the p-value, as the area to the right of \\(z=1.169\\) and see that \\(p \\approx 0.12\\). This represents the probability of observing a sample statistic this extreme (OR MORE) if the null hypothesis is true. 5) Draw a conclusion But what does it mean if you obtain a p-value that is incredibly tiny? If you obtain a p-value of .05 this means that the probability of obtaining a sample statistic as or more extreme than the one you obtained is only about 1/20 or 5%. This suggests that the null hypothesis is incorrect and that we should reject it. Generally, you decide upon an \\(\\alpha\\) or a level of significance before you perform the hypothesis test. The \\(\\alpha\\) tells you that for a p-value under the chosen value you will reject the null hypothesis. In this case, we have failed to reject the null hypothesis. That means, that there is not sufficient evidence in this sample to support your friends claim. You cannot reject the null hypothesis (which is not the same thing as supporting it). 27.6 Watch both of these videos A video about hypothesis tests (this is a two-sided test) A video about one-sided and two-sided hypothesis tests Large sample proportion hypothesis testing (WARNING: he uses different notation) "],
["type-i-and-type-ii-errors.html", "28 Type I and Type II Errors 28.1 Learning Objective: 28.2 Type I and Type II Errors", " 28 Type I and Type II Errors 28.1 Learning Objective: Recognize the difference between Type I and Type II errors 28.2 Type I and Type II Errors A type I error occurs when the null hypothesis is falsely rejected. We minimize the risk of making a type I error by using an \\(\\alpha\\) level as the basis of comparison for our p-value. For example, an \\(\\alpha\\) level of 0.05 means that only 1 in 20 times, we will make a type I error and falsely reject the null where the null hypothesis is a true descriptor of our data. A type II error occurs when the alternative hypothesis is true, but we fail to reject the null. Here is a table explaining the two types of errors we can encounter when interpreting a hypothesis test: ~ Truth=H0 Truth=Ha Fail to Reject H0=True No Error Type II Reject H0 Type I No Error Imagine a fire detector. The null state of being is “no fire.” The alternative hypothesis is “fire.” If the fire detector goes off but there is no fire (like when you take a hot shower in the traditional dorms), then a type I error has occurred- the null hypothesis of “no fire” was falsely rejected. Let’s say that your fire alarm keeps making a type I error, so you get frustrated and remove the batteries from the fire alarm. A few days later, there actually is a fire, but the alarm doesn’t go off because you took out the batteries. This means that a type II error has occurred- the alternative hypothesis was true (there was a fire), but instead you falsely failed to reject the null. In general, we are more concerned about Type I errors, since this will lead us to reject the null hypothesis when it is actually true. So, for instance, we might conclude that our experiment worked, when in fact the treatment had no effect. This video starts with a good example of two-sided large n hypothesis test (in case you need to refresh your memory), and at about the 3:00 mark, it explains the difference between type I and type II errors. "],
["hypothesis-testing-for-small-sample-quantitative-data.html", "29 Hypothesis Testing for Small Sample Quantitative Data 29.1 Learning Objective: 29.2 Hypothesis Test", " 29 Hypothesis Testing for Small Sample Quantitative Data 29.1 Learning Objective: Learn how to conduct a hypothesis test with a small number of observations for quantitative data. 29.2 Hypothesis Test 29.2.1 Step 1: Make assumptions about your data For these tests, we assume that our data is quantitative and that the population is normally distributed. We also know that our sample size is going to be relatively small, which means that we will be referencing the t table rather than the z table. 29.2.2 Step 2: Formulate null and alternative hypotheses Just like with other types of hypothesis testing, the null is what a determined skeptic would believe about what we are going to measure, and the alternative hypothesis is our research hypothesis based on the data collected. As with other types of hypothesis testing, this test can be either one-sided or two-sided. Make sure you keep in mind how you intend to test the data. 29.2.3 Step 3: Calculate a Test Statistic This statistic summarizes how much our alternative hypothesis differs from the rest of the data if the null was true. Here is the formula, which we’ve used many times before: \\[t^\\ast= \\frac{\\bar{x}-\\mu_0}{\\frac{s}{\\sqrt{n}}}\\] Don’t forget to note the degrees of freedom! d.f.= n-1 29.2.4 Step 4: Calculate a p-value Remember, a p-value is a measure of surprise, so small p-values more strongly contradict the null because that means it would be extremely surprising to see the results of our alternative hypothesis if the null were true. To find the p-value, go to this t table (click here), or we can use R. Keep in mind the confidence interval, the degrees of freedom, and if the hypothesis is one or two-sided. 29.2.5 Step 5: Draw a conclusion Are the results statistically significant given the pre-determined \\(\\alpha\\) level (usually is the p value &lt; 0.05)? If p&lt;\\(\\alpha\\), we reject the null and conclude that the evidence supports the alternative hypothesis. If \\(p&gt;\\alpha\\), we fail to reject the null. For more help, watch this video from the Khan Academy, which will walk you through a small-sample t-test step-by-step: "],
["review-exam-1.html", "30 Review Exam 1 30.1 Objectives 30.2 Random Samples 30.3 Measurement Error 30.4 Scales of Measurement 30.5 Measures of Position 30.6 Measures of Dispersion 30.7 Measures of Central Tendency 30.8 Sampling Distributions 30.9 The Normal Distribution, Binomial Distribution, and T-Distribution 30.10 Constructing Confidence Intervals 30.11 Hypothesis Testing 30.12 Notation, notation, notation 30.13 Some summarization", " 30 Review Exam 1 30.1 Objectives Remind yourself of what you have learned so far in QPM Understand the main themes and how they are related to one another Get an idea of the concepts and equations you will need to know for the exam Disclaimer This page is designed to be helpful as you review. It is absolutely not guaranteed to remind you of everything you need to study in order to get 100% on the exam. But this is definitely a good place to start. 30.2 Random Samples When we are dealing with a particular population and particular research questions, it is important that we have a random sample so our statistics are representative of the population from which we are taking the sample. There are various ways to put together a random sample, including cluster samples, stratified random samples, systematic samples, and multi-stage samples. Know what they are and how they are distinct from one another. To learn more: http://pages.wustl.edu/montgomery/articles/2527 30.3 Measurement Error Although we try our best to keep our samples random so that they are representative of our population, various types of measurement errors can occur in the form of bias. There are three in particular that are important to keep in mind: sampling bias (when the sample is not representative of the population), response bias (when subjects answer surveys in a way that is contrary to their own beliefs), and non-response bias (when subjects who are selected for a survey but don’t respond are different from the subjects who do respond in important ways). To learn more: http://pages.wustl.edu/montgomery/articles/2528 30.4 Scales of Measurement The data that we collect from our random samples, which are hopefully representative and non-biased, can take many forms depending on what variables we choose to study. There are four main types of scales you will need to know: nominal (categorical) variables are separated into different categories but have no natural ordering; ordinal variables have a natural ordering but do not have natural distances between their values; interval variables have both a natural ordering and a natural distance between values; and ratio variables are a subset of interval data in which the zero value signifies that there is none of that variable. Granularity refers to how well the data fits on a particular scale. A continuous variable can take on any value along a scale, while a discrete variable can take on only certain values. To learn more: http://pages.wustl.edu/montgomery/articles/2529 30.5 Measures of Position Once we have identified the scales that the variables will take, it will be useful to calculate some measures of position so that we can start to describe the data. These measures of position include percentile, interquartile range, outliers, and skew. The percentile tells us what percentage of the observations fall below a given observation. The interquartile range determines the middle 50% of the data. An outlier is an observation that has an extreme value. Skew is a way to describe the asymmetry of a dataset. To learn more: http://pages.wustl.edu/montgomery/articles/2579 30.6 Measures of Dispersion Another way we can start to describe data is through measures of dispersion that include range, deviations, variance, and standard deviations. The range is a simple measure of variability defined by the lowest value in a dataset subtracted from the highest value. A deviation is a measure of distance from the center, variance refers to the degree to which observations are spread out from the mean, and the standard deviation represents the distance from the mean. You should be able to define and calculate each of these measures of dispersion. To learn more: http://pages.wustl.edu/montgomery/articles/2578 30.7 Measures of Central Tendency Measures of central tendency are yet another way to describe data. These are the more traditional ways to describe data you have learned since middle school: mean, median, mode, etc. You should know how to calculate each. To learn more: http://pages.wustl.edu/montgomery/articles/2577 30.8 Sampling Distributions We have learned that we use samples to estimate parameters of a population. However, it is almost always the case that there is some error when we are trying to estimate population parameters using a sample. In order to know how much error will likely be in our sample, we use a sampling distribution, which uses probability theory to describe how sample statistics will vary. In other words, a sampling distribution is how the sample means or proportions are distributed. While we use the standard deviation to measure how far individuals in a sample tend to be from the sample mean, in a sampling distribution we use standard error to measure how far a sample mean or proportion tends to be from the true population mean or proportion. The Central Limit Theorem is an important rule you will need to know with regards to sampling distribution. It posits that when the sample sizes are sufficiently large, the resulting sampling distribution will be normally distributed, regardless of the population distribution. Furthermore, with an infinite number of samples, the mean of the sampling distribution will equal the true population mean. Familiarize yourself with sampling distributions and how they differ from sample distributions and population distributions. These are some of the most important concepts and distinctions you will need to know in this course. To learn more: http://pages.wustl.edu/montgomery/articles/2664 ; http://pages.wustl.edu/montgomery/articles/2666 30.9 The Normal Distribution, Binomial Distribution, and T-Distribution The standard normal distribution is probably the distribution you are most familiar with, and the one you will use most often. The binomial distributions and t-distributions are used in special cases-when the data calls for them. You should know what distinguishes each distribution from one another, when to use each, and how to calculate probabilities using the respective tables for both means and proportions. Examples are provided in the respective sections in the course book. To learn more: http://pages.wustl.edu/montgomery/articles/2678 ; http://pages.wustl.edu/montgomery/articles/2677; http://pages.wustl.edu/montgomery/articles/1841 30.10 Constructing Confidence Intervals Confidence intervals are a type of interval estimate of a population parameter and are used to indicate the reliability of an estimate. You should know exactly what they are and how to go about constructing them for means and proportions with large and small sample sizes. You should also know how to go about calculating the necessary sample size required for a desired confidence level. To learn more: http://pages.wustl.edu/montgomery/articles/2757 ; http://pages.wustl.edu/montgomery/articles/2795 ; http://pages.wustl.edu/montgomery/articles/2796 ; http://pages.wustl.edu/montgomery/articles/2772 30.11 Hypothesis Testing You should know the basics of hypothesis testing. The null hypothesis is that of the determined skeptic: the outcomes in each trial happened by chance and the world worked according to the laws of statistics, while the alternative hypothesis posits that something more than chance is acting on a population. You should understand the concept of a p-value and be able to interpret the results of a hypothesis test. You should be able to conduct a significance test for a mean and a proportion with both large and small sample sizes. There are five steps in conducting a hypothesis test: Make assumptions about your data. Identify the null and alternative hypotheses. Is the significance test one-sided or two-sided? Calculate a test statistic. Identify the P-value. Form a conclusion. You should also be able to recognize Type I and Type II error. Type I error occurs when the null hypothesis is true and you’ve rejected the null hypothesis (an alarm without a fire). Type II error occurs when the null hypothesis is false and you fail to reject the null hypothesis (a fire without an alarm). To learn more: http://pages.wustl.edu/montgomery/articles/2898; http://pages.wustl.edu/montgomery/articles/6203; http://pages.wustl.edu/montgomery/articles/2977; http://pages.wustl.edu/montgomery/articles/2978 30.12 Notation, notation, notation ~ Mean Standard deviation Estimated mean (using sample distribution) Estimated standard deviation (using sample distribution) Sample distribution \\(\\bar{x}\\) \\(s\\) N/A N/A Sampling distribution \\(\\mu_{\\bar{x}}\\) \\(\\sigma_\\bar{x}\\) (Called standard error) \\(\\hat{\\mu}_{\\bar{x}}\\) \\(\\hat{\\sigma}_{\\bar{x}}\\) Population distribution \\(\\mu\\) \\(\\sigma\\) \\(\\hat{\\mu}\\) \\(\\hat{\\sigma}\\) ~ Expected Value Standard deviation Estimated Proportion (using sample distribution) Estimated standard deviation (using sample distribution) Sample distribution \\(\\hat{\\pi}\\) (Not needed)) N/A N/A Sampling distribution \\(\\mu_\\hat{\\pi}\\) \\(\\sigma_{\\hat{\\pi}}\\) \\(\\hat{\\mu}_{\\hat{\\pi}}\\) \\(\\hat{\\sigma}_{\\hat{\\pi}}\\) Population distribution \\(\\pi\\) \\(\\sigma=\\sqrt{\\pi(1-\\pi)}\\) \\(\\hat{\\pi}\\) \\(\\hat{\\sigma}\\) 30.13 Some summarization "],
["causation-and-average-treatment-effects.html", "31 Causation and Average Treatment Effects 31.1 Learning Objectives: 31.2 Introduction 31.3 So, what exactly is causation? 31.4 How do we determine what the counterfactual is? 31.5 How does an experiment work? 31.6 So, how do we measure the effect of the treatment in an experiment? 31.7 But what if we can’t do an experiment? 31.8 Confounding relationships 31.9 Conclusion", " 31 Causation and Average Treatment Effects 31.1 Learning Objectives: Understand how social scientists define causation Understand the fundamental problem of causal inference Understand why experiments are social scientists’ ideal method of determining causation Understand the term average treatment effect Understand the difficulties social scientists face when using correlation in the absence of an experiment to establish causality. 31.2 Introduction So far, we’ve talked about univariate inference. This is useful, say, if we wanted to conduct a poll to determine what proportion of the country intends to vote Republican in the next election, or to determine the average household income in a certain country. But there is one crucial part of social science that this class has not really addressed: how to determine cause and effect. Causation is a crucial part of the social sciences (and of science in general). Political scientists aren’t just concerned with describing a certain factor in the world (income, political view); they’re concerned with understanding how those factors relate to each other. Does income inequality cause political polarization? Does being democratic make a country less likely to go to war? Does being appointed by a Republican make a judge more likely to vote consistent with conservative policies? 31.3 So, what exactly is causation? Imagine you’re driving a car down a four-lane road. All of the sudden, out of nowhere, a car drives out of its lane, into yours, right in front of your car. You can tell that the driver is texting, and isn’t looking at the road. You slam on the breaks, and your hood collides with his fender. Fortunately, you’re shaken, but fine, and so is the other driver. Chances are, you will be asking yourselves some questions. Why did that happen? What would have happened if I had been paying more attention to the cars aroudn you? What if that driver hadn’t been texting? You would be asking yourself what caused that accident. What does it mean for something to have caused that accident? What things can we point to? Well, we’d probably say that the other driver caused the accident. Or maybe, if we wanted to be precise, we’d say that the fact that the driver was texting while driving caused the accident. But what does it mean when we say that the fact that the driver was texting caused the accident? And what makes the fact that the driver was texting while driving any different than the fact that the sky was blue before the accident? After all, both are true statements. But only one could reasonably be considered a “cause” of the accident. Philosophers and scientists have pondered the question of cause and effect for centuries, and have come up with various different definitions of causation. But the definition that is generally used by social scientists is this: a X causes Y if Y would not have occurred but for X. One of the most famous examples of this in popular culture occurs in the movie “It’s A Wonderful Life.” In the movie, the protagonist, George Bailey, in a moment of despair, wishes that he had never existed. Following this, his guardian angel shows him what his community would look like if he had never existed. As George Bailey walks around this new version of the world, he directly sees what he has caused, because the world he sees is a world in which everything is the same except for one thing: he was never born. Because of this, every difference between the actual world and this hypothetical world can be attributed to his birth and life. In science, the term for what would be if it were not for the existence of some causal factor-is called the counterfactual. For example, if we wanted to know whether the stimulus package of 2008 created jobs, then the counterfactual would be what would have happened if the stimulus package had not been passed. So to estimate the effect of the stimulus, we have to know what would have happened without it. If that counterfactual is that unemployment would have increased to 20% in the absence of stimulus, then we could conclude that the stimulus package was effective in creating jobs. If the counterfactual was that unemployment would have gone down to 2% in the absence of stimulus, then we could conclude that the stimulus package failed. So your estimate of the effect of the stimulus depends entirely on what you assume would have happened in the absence of stimulus. 31.4 How do we determine what the counterfactual is? As you may have figured out, it’s actually impossible for us to know what the counterfactual is, because, logically, X cannot simultaneously be true and not true. Either the incumbent wins an election, or she doesn’t. Either a household experiences tax increases, or it doesn’t. Either the stimulus is put into effect, or it isn’t. Either the other driver was texting or he/she wasn’t. We do not have the luxury of being able to see the results of both scenarios play out simultaneously. This is sometimes called the fundamental problem of causal inference. We can observe only the world in front of us. We cannot observe hypothetical counterfactuals But do not lose hope. There are several ways scientists have developed to estimate causal effects that can partially overcome this problem. The method that most directly determines cause is an experiment. 31.5 How does an experiment work? Very basically, in an experiment, we isolate the one variable whose effect we want to measure. We create two groups, which are (ideally) identical in every single way, except one group is exposed to that variable (called the treatment group), while the other is not (called the control group). In order to get the two groups to be as similar as possible, we use randomization. That is, we randomly assign people to be either in the control or treatment groups. For example, if we wanted to study the impact of a certain medication on heart pressure, then we would randomly assign the participants to either receive the medication or receive a placebo. If the treatment group improved, and the control group didn’t, we would be more confident that the cause of this improvement was the medication. 31.6 So, how do we measure the effect of the treatment in an experiment? Imagine you’re running an experiment on political views. Let’s say you want to know whether exposure to negative campaign ads makes someone less likely to believe that politicians care about them. You randomly assign half of your subjects to watch a negative campaign ad, and half of your subjects watch the same clip, altered slightly to change the tone from negative to neutral. You then ask the subjects to fill out a survey, evaluating their attitudes towards politics. If your experiment works the way any other experiment does, there’s probably a lot of statistical noise. Even if your experiment worked, it’s unlikely that every single person in the treatment group scored lower than every single person in the control group. There’s probably a decent amount of variance in both groups. So how do we measure the effect of the experimental treatment? It’s actually somewhat straightforward: we measure the difference between the average score for the treatment group, and the average score for the control group. This will give us something we call the average treatment effect (ATE). To state it more formally, the formula for the ATE is: \\[\\frac{1}{N}\\sum_{i=1}^N (Y_i(1) - Y_i(0)) \\] Where N is the number of subjects, \\(Y_i(1)\\) is the outcome if the observation is in the treatment group, and \\(Y_i(0)\\) is the outcome variable in the control group. As an example, imagine that individuals 1,2,3 are assigned to the treatment group, and individuals 4 and 5 are assigned to the control group. \\[ATE=\\frac{Y_1+Y_2+Y_3-Y_4-Y_5}{5}\\] (NOTE: We haven’t yet covered regression, but when we do, something to keep in mind: the beta coefficient in a regression is that ATE for that variable.) 31.7 But what if we can’t do an experiment? Especially in the social sciences, we cannot experimentally manipulate everything. We cannot randomly have certain countries go to war with each other, and others not, to see what the impact is. We cannot force certain counties to elect Democrats, and others to elect Republicans. In those cases, we cannot use experimental methods to evaluate causal claims. So instead, we use some statistical methods to try to untangle causation. When we can’t use experimental control, then we collect data, and try to determine cause using statistical control. When we are trying to figure out whether X caused Y, there are 3 things we want to do: 1) Establish temporal order For X to have caused Y, X must have come before Y. This is relatively straightforward. BUT, just because X came before Y, doesn’t mean that X caused Y. 2) Show an association This is what we will focus on the rest of the semester, using regression and a number of other statistical techniques. Basically, you have to show that there is some sort of relationship between two variables. BUT, just because X is associated with Y, doesn’t mean X caused Y. Correlation is not causation. For example, ice cream sales are close related to drowning deaths (i.e., when ice cream sales rise, so do drowning deaths). Of course, that doesn’t mean that by ice cream sales cause drowning deaths; both just occur in the summer. 3) Eliminate alternative explanations We want to show that no other factor explains the outcome that we are trying to explain. Once we have done these three things, then we can say that X caused Y. But doing this is not easy. In particular, eliminating alternative explanations is very difficult because of something we call confounding. This brings up an important concept known as endogeneity. That basically just refers to when one variable in a model is correlated with another, like how the time of year predicted ice cream sales, but there actually is no causal relationship. 31.8 Confounding relationships So, what other relationships can exist other than a direct causal relationship? There could be a spurious relationship between the two variables (let’s call them X and Y), where some third variable (let’s call it Z) is actually the causal factor. \\(Z \\rightarrow X\\) and \\(Z \\rightarrow Y\\) Spurious relationships are especially problematic, because they will result in an association between two variables that is not causal. There could be a chain relationship, where: X \\(\\rightarrow\\) Z and \\(Z \\rightarrow Y\\) There could be multiple causal relationships, where: \\(X \\rightarrow Y\\) and \\(Z \\rightarrow Y\\) Finally, there could be reverse causality, where, \\[Y \\rightarrow X\\] 31.9 Conclusion X causes Y if Y would not have occurred if not for X. Ideally, to establish causation, we use experiments, in which we manipulate X, and see if Y occurs. We use random assignment to ensure that the only difference between the treatment and control groups is the treatment. If we cannot use experiments, then we have to rely on other methods to establish correlation. But because correlation does not imply causation, we need to establish temporal order, establish association, and rule out other explanations. This is much more difficult, and requires more advanced statistical techniques. "],
["comparing-means.html", "32 Comparing Means 32.1 Comparing Means for Quantitative Variables 32.2 Learning Objective #1: What Information Goes Into Comparing Means for Quantitative Variables? 32.3 Learning Objective #2: What Formulas are Needed for Comparing Means for Quantitative Variables? 32.4 Learning Objective 3: Putting it all into practice! 32.5 What did you learn?", " 32 Comparing Means 32.1 Comparing Means for Quantitative Variables You will learn how to compare means between two different samples. This should make you all very excited; because it gives you the ability to answer some pretty cool types of questions. For example: Does studying actually get me a better grade on a test? If I eat more sugar, will that make me gain weight? Do people become more partisan as they age, or more moderate, or neither?! The reason that you will be able to answer these questions is we will show you how to compare the means of two groups, to see if they can plausibly be equal. It is important to note that this section will work only for means, not proportions. 32.2 Learning Objective #1: What Information Goes Into Comparing Means for Quantitative Variables? In order to make this comparison, you need 4 pieces of information: The mean for sample 1 (\\(\\bar{y}_1\\)) The mean for sample 2 (\\(\\bar{y}_2\\)) The standard deviation for sample 1 \\(S_1\\) The standard deviation for sample 2 \\(S_2\\) 32.3 Learning Objective #2: What Formulas are Needed for Comparing Means for Quantitative Variables? Hypothesis Testing About Differences: ((Estimate-Null)/Standard Error) Estimate=\\((\\bar{y}_1 - \\bar{y}_2)\\) Null = generally zero (that is, you are guessing that there is no difference between the two samples) If the sample size is large then there will be a corresponding Z-Score. The p-value will be calculated just as we have done with hypothesis testing previously. And Standard Error will be calculated as follows: \\[ SE= \\sqrt{\\frac{S_1^2}{n_1} +\\frac{S_2^2}{n_2}} \\] If the sample size is small, then all calculations will be the same except for two differences. First, the calculation for Standard Error will be as follows : \\[Standard Error=\\hat{\\sigma}\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}\\] where \\(\\sigma^2\\) is the pooled variance, which is calculated as \\[\\hat{\\sigma}=\\sqrt{\\frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1+n_2-2}}\\] Second, the “test statistic” must be taken from the T-Score table, with the Degrees of Freedom= \\((n_1+n_2-2)\\) 32.4 Learning Objective 3: Putting it all into practice! Step 1: Assumptions Either Large “N” or Small “N” (If N is less than 30, use Small “N”) If n is small, we must must be able to assume a Normal Distribution Null and Alternative Hypotheses For both Small and Large “N” for differences of the mean, the “Null” would be that the means are the same \\((\\mu_1-\\mu_2=0)\\), and the alternative would be that the means are not equal\\((\\mu_1-\\mu_2\\ne0)\\) Calculating a Test Statistic Formula: (Estimate-Null)/Standard Error First, you calculate the difference of the means (estimate). This merely involves subtracting the sample mean of one sample from the sample mean of the other sample Then you must calculate the Standard Error For Large Sample Size: Formula for SE: \\(\\sqrt{\\frac{S_1^2}{n_1} +\\frac{S_2^2}{n_2}}\\) Use Z-score For Small Sample Size Formula for SE: \\(\\hat{\\sigma}\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}\\), where \\(\\hat{\\sigma}\\) is the pooled standard deviation above. Use T-Score The value for the Null is usually “0” Calculating P-Value For Small Sample Size Take the absolute value of the test statistic Find Degrees of Freedom: Formula: DF=N1+N2-2 Find the corresponding probability of the test statistic or more extreme, given the degrees of freedom If the Alternative Hypothesis is Sample 1 does not equal Sample 2 (as opposed to, say, Sample 1 is less than Sample 2) then you must multiply the probability by 2 For Large Sample Size Take the absolute value of the test statistic Find the corresponding probability of the test statistic or more extreme If the Alternative Hypothesis is Sample 1 does not equal Sample 2 (as opposed to, say, Sample 1 is less than Sample 2) then you must multiply the probability by 2 Draw a Conclusion Choose an Alpha, generally 0.01, 0.05, or 0.10 in Political Science If the P-value is larger than the alpha level, you will fail to reject the null hypothesis If the P-value is smaller than the alpha level you will reject the null hypothesis In these videos, Kahn discusses the sampling distribution for the difference of means from two samples and shows how the calculate a confidence interval for that quantitity. He then goes on to show how to conduct a hypothesis test (which, as we know, is a very related task). But watch all of the videos, as I think they are worthwhile for understanding the key points discussed above. Here, he uses a large “N” and therefore a Z-score. If “N” were small, the only difference would be that you would need to calculate P-value using a T-score (with the correct standard error equation), and consider the Degrees of Freedom. 32.5 What did you learn? Goal 1: Learning what goes into comparing two samples of quantitative variables Goal 2: Understanding the formulas for performing a hypothesis test comparing two samples of quantitative variables with both small and large sample sizes Goal 3: Understanding how to perform a hypothesis test comparing two samples of quantitative variables with both small and large sample sizes "],
["t-tests-in-r.html", "33 T-Tests in R", " 33 T-Tests in R "],
["chi-squared-test-of-independence.html", "34 Chi Squared Test of Independence 34.1 Learning Objective 1: What is a Chi-Squared Test of Independence? 34.2 Learning Objective 2: Performing a Chi-Squared Test of Independence 34.3 Learning Objective 3: Seeing How it is Done!", " 34 Chi Squared Test of Independence 34.1 Learning Objective 1: What is a Chi-Squared Test of Independence? When you are given a contingency table (which deals with two variables), you may want to know whether the results indicate that the two variables are related. That is, you might want to see if the two variables are independent of one another. If the variables are independent, then a change in one should not be related with change in the other. If they are dependent, one variable will change in concert with the other. In the Chi-Squared Test, the goal is to compare the values in a contingency table that would be expected if the variables are independent with the values we actually observe. If the observed numbers differrs “too much” from what we would expect if they are independent, we can reject the null hypothesis of independence. 34.2 Learning Objective 2: Performing a Chi-Squared Test of Independence To start, you will need a contingency table. You must have the responses tallied in each row and column with the aggregate totals for each row and column as well. Here is an example of table that looks at how age is related to preferences for hamburgers or hotdogs. ~ Hamburger Hotdog Total Under 30 25 13 38 Over 30 22 13 35 Total 47 26 73 Then you will need to specify your explanatory and outcome variables Explanatory variable: The variable that is statistically independent. For instance, in a medical experiment this might be some new drug. Outcome variable: The variables that might be statistically dependent. In experiment, for example, this would be the desease you are trying to cure. Our null hypothesis is that the distribution of the outcome variable will not change as a function of the value of the explanatory variable. 3a. Then you must calculate the \\(\\chi^2\\) statistic: First you must find \\(f_{observed} = f_o\\) = observed frequency = the raw count (NOT THE %). This is the number of cases we observe in each cell of the table. Then you must find $f_{expected} = f_e $= what we would expect for independent samples. This value is equal to: \\[\\frac{\\text{Row total }\\times \\text{Column total}}{\\text{Grand total}}\\] If the null hypothesis true, then we would expect \\(f_{observed} = f_{expected}\\) The formula for the \\(\\chi^2\\) measures the degree to which our obsrerved data differs from what we would expect under the null. Specifically, \\[\\chi^2 = \\sum\\frac{(f_0-f_e)^2}{f_e}\\] Note that we summing over all cells in the table. 3b. A few comments on \\(\\chi^2\\): If the samples are truly independent, \\(\\chi^2 \\rightarrow 0\\). If they are actually dependent, \\(\\chi^2 \\rightarrow \\infty\\) This statistic is distributed according to the \\(\\chi^2\\) distribution. And we will use that to calculate p-values. YOU WILL ALWAYS HAVE A POSITIVE NUMBER. Once you get the \\(\\chi^2\\) statistic you must find the Degrees of Freedom to calcualte the p-value. DF= (rows ??? 1)(columns ??? 1) In R, the code is: pchisq(\\(\\chi^2\\), df = (rows-1)(columns-1), lower.tail=FALSE) Use this table for exams. Conclusion If your P-value is greater than your \\(\\alpha\\), you will fail to reject the null hypothesis. If it is less than your \\(\\alpha\\), you will reject the null hypothesis. 34.3 Learning Objective 3: Seeing How it is Done! First let’s pose a question: Is age independent of preference between hamburgers and hotdogs Now, let’s look at a chart which gives responses for an actual example. Here is the contingency table from above. ~ Hamburger Hotdog Total Under 30 25 13 38 Over 30 22 13 35 Total 47 26 73 Now let’s get Null and Alternative Hypotheses: Null: Age has no impact on preference between Hotdogs and Hamburgers Alternative: Age does have an impact on preference between Hotdogs and Hamburgers From there, it is time to calculate the Chi Squared statistic First, you must calculate the expected (\\(f_e\\)) values for each of the cells using the following formula above. You calculate it for all of the “response” cells. The \\(f_e\\) for each cell are provided in each cell in parenthesis. ~ Hamburger Hotdog Total Under 30 25(24.5) 13(13.5) 38 Over 30 22(22.5) 13(12.5) 35 Total 47 26 73 Second, you must calculate chi squared statistic with the following formula: \\[\\chi^2 = \\sum\\frac{(f_0-f_e)^2}{f_e}\\] If you do the calculation, you should find 0.061 Now you need the degrees of freedom. DF= (Row-1)(Column-1)=(2-1)(2-1)=1 Note: You might be tempted to say that the # of rows=4 and the # of columns=4. However, we are not looking for the total number of rows and columns in the table, just the number of rows and tables for the “response” cells! Now you calculate the p-value: Given the test statistic of 0.061 and df=1, P-value should come out to around 0.8. This is a very high p-value, and therefore you will fail to reject the null hypothesis, and therefore age does not seem to be related to hamburger/hotdog preference. Below is a video, which will take you through the same type of calculations that were performed above Contingency Table Chi-Square Test: Contingency Table Chi-Square Test "],
["bivariate-regression.html", "35 Bivariate Regression 35.1 Learning Objectives: 35.2 Summary 35.3 What is a linear regression? 35.4 When do we use linear regression?", " 35 Bivariate Regression 35.1 Learning Objectives: Understand what linear regression is Understand what linear regression is used for Know how to calculate a linear regression 35.2 Summary Linear regression is a type of model used to describe the relationship between two interval variables. The linear regression is the straight the line for which the sum of the squared distances between the data point and the expected value along that line is minimized. A linear regression can tell us whether there’s an association between two variables and how strong the relationship is. 35.3 What is a linear regression? So far, the tools we have covered can help us tell whether there is a difference between two groups (e.g., a treatment and control group). Now we’re going to cover a tool that helps us determine the relationship between two variables in one population. For example, let’s say we want to look at the relationship between age and political ideology. A linear regression is a line that describes the “best fit” for the observed data. We will talk about how we determine what that best fit is in the next section, but right now, let’s talk about the components of a line. A line can be written as where \\(\\beta\\) is the slope of the line, and \\(\\alpha\\) is the y-intercept (the value of y when x is zero). If the line is horizontal, then the slope would be zero. The slope is given as the change in y divided by the change in x, or “rise over run.” In the line above, the y intercept is .8, and the slope is \\(\\frac{(4-2)}{(8-3)} = \\frac{2}{5}\\). Therefore, the equation of the line above can be given as \\(y=.8+.4 \\times x\\) . 35.4 When do we use linear regression? Imagine you want to determine the relationship between the average income in a county, and the percentage of voters in that county who vote. In this case, there are two interval variables: an average income and a percent. Imagine that your hypothesis is that having a higher income causes voter turnout. You would want to have some way to measure the relationship between these two variables. You might select a random sample of counties in the United States, and determine the average income in that county, as well as the proportion of voters in that county. Then what? Linear regression is the most commone way that researchers determine the relationship between two variables. Remember the parts of a line. In the equation for a line, there are two variables, x and y. According to your hypothesis, voter turnout is the dependent variable, and income is the independent variable. We would let y represent the dependent variable, and let x represent the independent variable. This means that we have a line that tells us that \\[Turnout = \\alpha + Income \\times \\beta.\\] Your theory would lead you to expect that \\(\\beta\\ne0\\) (that there is a relationship between turnout and income). The problem then becomes, how do we determine the “correct” values of \\(\\alpha\\) and \\(\\beta\\)? To answer that question, we have to take a step back, and note that, unless one variable perfectly predicts another (which is extremely rare in social science), your data points are unlikely to all fall exactly on one line. This means that there is some error inherent in linear models. When we take into account error, \\(\\epsilon\\), we get the following formula: \\[y = \\alpha + x \\beta + \\epsilon \\] In order to use a linear regression, we have to assume that the errors are normally distributed. That is very, very important. When we say that errors are normally distributed, what we mean is that if you were to plot the distance from the actual y-value to the expected y-value based on the line, the resulting plot would be a normal distribution with a mean at 0, and a standard deviation of \\(\\sigma^2\\). Fitted values This means that , our best guess for \\(y\\) based on the regression and some observed value of \\(x\\), is given by the equation \\[ \\hat{y_i} = \\alpha+x_i \\beta\\] Estimating \\(\\alpha\\) and \\(\\beta\\) So how do we know what numbers to plug in for \\(\\alpha\\) and \\(\\beta\\)? We plug in the numbers that result in the lowest sum of squared error. To calculate this, we use the Sum of Squared Error (SSE). The equation is: \\[SSE = \\sum_{i=1}^n(y_i - \\hat{y_i})^2 = \\sum (y_i - \\hat{\\alpha} - x_i\\hat{\\beta})^2\\] We use this equation to determine the value of \\(\\alpha\\) and \\(\\beta\\) for which the SSE is lowest. The solution is the two following equations \\[\\hat{\\beta} = \\frac{\\sum_{i=1}^n\\Big((X_i - \\bar{X})(Y_i - \\bar{Y})\\Big)}{\\sum_{i=1}^n(X_i - \\bar{X})^2}\\] \\[ \\hat{\\alpha} = \\bar{Y} - \\hat{\\beta}\\bar{X}\\] The video below does a very thorough job of breaking down how to perform a linear regression. Here’s another video on how linear regressions work. It’s a little shorter. "],
["inference-for-regression.html", "36 Inference for Regression 36.1 Learning objectives: 36.2 Inference with regression 36.3 Table Interpretation", " 36 Inference for Regression Has a professor ever given you a study to read as homework with a confusing table with numbers, asterisks, and letters? Today, we are going to teach you to understand that table so you can impress your friends, relatives, and (most importantly) your professors with your mastery of regression analysis. 36.1 Learning objectives: Be able to make inferences from regression using hypothesis tests, t-statistics, p-values, and confidence intervals. Be able to interpret a regression table. 36.2 Inference with regression 36.2.1 Hypothesis tests We can use hypothesis tests to determine whether or not a relationship exists between two variables in our prediction equation. Our null hypothesis is that no relationship exists between the two variables. This occurs when the slope describing the relationship between x and y, \\(\\beta\\), is zero, or a flat line. This is described by \\(H_0: \\beta = 0\\), while the alternative hypothesis Ha can either be \\(H_a: \\beta \\neq 0\\) or \\(\\beta &lt; 0\\) or \\(\\beta &gt; 0\\), depending on what we are trying to assess. To calculate our test statistic, we subtract our calculated slope value, \\(\\beta\\), from our null hypothesis value and divide by the standard error. That is: \\[ t = \\frac{\\beta - 0}{se} \\] The formula for the standard error of the slope is: \\[ se = \\frac{s}{\\sqrt{ \\sum(x_i - \\bar{x})^2)}} \\] where \\[ s = \\sqrt{\\frac{SSE}{n-2}} \\] The test statistics, \\(t\\) is distributed according to the t distribution. We calculate the degrees of freedom by subtracting from n the number of parameters we are estimating. Since here we only have a bivariate regression, we would have degrees of freedom \\(= n -2\\) (one for \\(\\alpha\\) and one for \\(\\beta\\)). As always, if \\(n&gt; 30\\) we can approximate the t distribution with the z distribution. Again as always, the p value describes the probability of obtaining the observed slope were the true slope describing the relationship between the two variables were zero and the null hypothesis were true. It is our measure of surprise. A small p value suggests the regression line has a non-zero slope. That is, if the slope were actually zero, than the strength of the relationship between x and y would be very surprising. If the p value is less than our predetermined \\(\\alpha\\) (DONT GET CONFUSED … this is not the same \\(\\alpha\\) as above) we can reject the null hypothesis that there is no relationship between the two variables. 36.2.2 Confidence intervals We do not just want to know, however, that the slope is non-zero and that a relationship exists between the variables. We want to know the extent of that relationship, which we can determine using confidence intervals. We calculate the confidence interval for the slope using the formula: \\[ \\beta \\pm t*(se) \\] with \\(df = n - 2\\) and the \\(se\\) calculated exactly as it is in the above hypothesis test. Let’s do an example. In a data set of size n= 100 where x = size of house in square feet and y = selling price, our observed slope is 126.6, meaning that for each square foot increase in the size of the house, the selling price increases by 126.6 dollars (on average). Our \\(se\\) equals 8.47. In a 95% confidence interval for the true slope value \\(\\beta\\), we calculate with \\(df = 100 - 2\\) and \\(t = 1.984\\): \\[ 126.6 \\pm 1.984 (8.47) = 126.6 \\pm 16.8 = [110, 143]\\] If we were to take many, many samples of size \\(n = 100\\), we would be confident that 95% of the time that the true mean increase in selling price for each 1 square foot increase in size would fall between 110 dollars and 143 dollars. 36.3 Table Interpretation In your readings for your other Poli Sci classes have you ever come across a chart like this? Well, now you have the tools to interpret what it means. For the moment, ignore the fact that there are multiple variables. We will come back to that later in the class. Imaging that this is a bivariate regression with only one explanatory variable and a constant. For example, dependent variable in this chart is a house incumbent’s legislative electoral vote share from 1980 - 1996 (the left panel). The first explanatory variable listed here is roll-call ideological extremity. The first coefficient listed is the slope. It shows us that the relationship between incumbent vote share decreases by .085 for each increase in roll-call ideological extremity. The number in parentheses is the standard error for that \\(\\beta\\). It tells us the variability in sample slope values that would result from repeatedly selecting random samples of incumbent vote share and calculating prediction equations. The stars next to the slope coefficient for roll call extremity shows us that the relationship between incumbent vote share and roll call ideology is significant at the \\(\\alpha = .01\\) or the \\(\\alpha = .001\\) level (see the note at the bottom of the table). "]
]
